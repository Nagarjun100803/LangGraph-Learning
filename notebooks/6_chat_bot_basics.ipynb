{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f874527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shivaya Namah\n"
     ]
    }
   ],
   "source": [
    "print(\"Shivaya Namah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62beb699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded sucessfully.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_settings import  BaseSettings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "class Settings(BaseSettings): \n",
    "\n",
    "    pinecone_api_key: str \n",
    "    watsonx_apikey: str  \n",
    "    watsonx_url: str\n",
    "    watsonx_project_id: str\n",
    "    cohere_api_key: str \n",
    "    tavily_api_key: str\n",
    "    google_api_key: str\n",
    "    groq_api_key: str\n",
    "\n",
    "    langsmith_api_key: str\n",
    "    langsmith_endpoint: str\n",
    "    langsmith_tracing: bool\n",
    "    langsmith_project: str\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        extra = \"ignore\"\n",
    "\n",
    "settings = Settings()\n",
    "print(\"API keys loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400b975d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000234B77AFE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000234B7938AD0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key = settings.groq_api_key,\n",
    "    temperature = 0.7,\n",
    "    max_retries = 2,\n",
    "    max_tokens = 2000,\n",
    "    model = \"llama-3.1-8b-instant\"\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a363f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Nagarjun. Is there something I can help you with or would you like to chat?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 41, 'total_tokens': 65, 'completion_time': 0.032, 'prompt_time': 0.449172806, 'queue_time': 0.04986988000000003, 'total_time': 0.481172806}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--79df26e3-14c0-4b84-965c-f21f5ef4eb4c-0' usage_metadata={'input_tokens': 41, 'output_tokens': 24, 'total_tokens': 65}\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 708 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm.invoke(\"Hi, this is Nagarjun\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af996a",
   "metadata": {},
   "source": [
    "### Create a simple ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369c3a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000234B7B46E40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, add_messages\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class BasicChatBotState(TypedDict):\n",
    "    \"Represents the state of basic ChatBot\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def chatbot_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"Call the llm and and get the answer.\"\n",
    "    return {\n",
    "        \"messages\": [llm.invoke(state[\"messages\"])]\n",
    "    }\n",
    "\n",
    "CHATBOT = \"chatbot\"\n",
    "\n",
    "graph_builder = StateGraph(BasicChatBotState)\n",
    "graph_builder.add_node(CHATBOT, chatbot_node)\n",
    "graph_builder.set_entry_point(CHATBOT)\n",
    "graph_builder.add_edge(CHATBOT, END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6845a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"What do you think about using FastAPI with LangGraph.\")\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dcbaf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FastAPI with LangGraph can be a powerful combination for building a robust and scalable natural language processing (NLP) application. Here's a high-level overview of the benefits and considerations:\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "1. **FastAPI's strengths complement LangGraph**: FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints. LangGraph, on the other hand, is a Python library for building language models and processing natural language data. Combining the two can take advantage of FastAPI's performance and LangGraph's NLP capabilities.\n",
      "2. **Streamlined API development**: With FastAPI, you can quickly create APIs for interacting with your LangGraph models, making it easier to integrate NLP functionality into your application.\n",
      "3. **Easy deployment**: FastAPI applications can be deployed to various platforms, such as Kubernetes, Docker, or directly with `gunicorn`.\n",
      "\n",
      "**Considerations:**\n",
      "\n",
      "1. **Performance**: While FastAPI is designed for high-performance, the actual performance of your application will depend on the complexity of your LangGraph models and the volume of requests you receive.\n",
      "2. **Memory usage**: LangGraph models can consume significant memory resources. Make sure your system has sufficient memory to handle the models and incoming requests.\n",
      "3. **Error handling**: When using LangGraph with FastAPI, ensure that you handle errors and exceptions properly to prevent crashes and provide meaningful error messages to users.\n",
      "4. **Model training and maintenance**: While FastAPI can handle API requests, you'll still need to train and maintain your LangGraph models separately. Consider using a separate process or service for model training and updating.\n",
      "\n",
      "**Example use case**:\n",
      "\n",
      "Suppose you want to build a language translation API using LangGraph and FastAPI. You can define a FastAPI endpoint that accepts text input, uses LangGraph to translate the text, and returns the translated text in a JSON response.\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from langgraph import LangGraph\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Create a LangGraph instance\n",
      "langgraph = LangGraph(lang='en', model='transformer')\n",
      "\n",
      "@app.post(\"/translate\")\n",
      "async def translate(text: str):\n",
      "    # Use LangGraph to translate the text\n",
      "    translation = langgraph.translate(text)\n",
      "    return {\"translation\": translation}\n",
      "```\n",
      "\n",
      "In this example, the `translate` endpoint accepts a string input, uses LangGraph to translate the text, and returns the translated text in a JSON response.\n",
      "\n",
      "**Conclusion**:\n",
      "\n",
      "Using FastAPI with LangGraph can be a great combination for building NLP applications. While there are some considerations to keep in mind, the benefits of streamlined API development, easy deployment, and high-performance capabilities make it an attractive choice for many use cases. Just be sure to carefully plan and implement error handling, memory management, and model training and maintenance to ensure a smooth and efficient application.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb4c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"What is LangGraph and why it is popular? Also tell me your cutoff date.\")\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728fd7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cutoff date for knowledge is December 2023. \n",
      "\n",
      "LangGraph is a graph-based language model developed by the research team of the University of Cambridge. It is a type of graph neural network that can capture complex relationships between words and their meanings. LangGraph uses a graph structure to represent the relationships between words, allowing it to understand nuanced semantic relationships and nuances of language.\n",
      "\n",
      "LangGraph is popular in the research community for several reasons:\n",
      "\n",
      "1. **Improved representation of semantic relationships**: LangGraph's graph-based representation allows it to capture complex relationships between words that are not easily captured by traditional word embeddings.\n",
      "\n",
      "2. **Better handling of out-of-vocabulary words**: LangGraph's ability to learn from context allows it to generalize and handle out-of-vocabulary words more effectively.\n",
      "\n",
      "3. **Scalability**: LangGraph is designed to be scalable, making it suitable for large-scale natural language processing tasks.\n",
      "\n",
      "However, LangGraph is still a relatively new development in the field, and its popularity may vary depending on the specific application and research community.\n",
      "\n",
      "Additionally, LangGraph is still a topic of ongoing research and development, and its applications and advantages are being explored and refined by researchers in the field.\n",
      "\n",
      "If you have more specific questions or need further clarification, please let me know!\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b005e4e",
   "metadata": {},
   "source": [
    "### Basic Chatbot With Some Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ca6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class InputSchema(BaseModel):\n",
    "    \"\"\"\n",
    "    InputSchema defines the input structure for numerical operations, containing two fields:\n",
    "    - num_1: The first number (integer or float).\n",
    "    - num_2: The second number (integer or float).\n",
    "    \"\"\"\n",
    "    num_1: int | float = Field(description = \"The first number (integer or float) eg. 8 or 8.5\")\n",
    "    num_2: int | float = Field(description = \"The second number (integer or float) eg. 8 or 8.5\")\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name_or_callable = \"date_time\", \n",
    "    description = \"Use when you need to get current date and time.\",\n",
    ")\n",
    "def get_current_date():\n",
    "    \"\"\"Returns the current Date.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %I:%M:%S, %p\")\n",
    "\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name_or_callable = \"addition\",\n",
    "    description = \"Use when you need to perform addition operation.\",\n",
    "    args_schema = InputSchema\n",
    ")\n",
    "def addition(num_1: int | float, num_2: int | float) -> float:\n",
    "    \"\"\"Returns the addition of two numbers.\"\"\"\n",
    "    return num_1 + num_2\n",
    "\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name_or_callable = \"multiplication\",\n",
    "    description = \"Use when you need to perform multiplication operation.\",\n",
    "    args_schema = InputSchema\n",
    ")\n",
    "def multiplication(num_1: int | float, num_2: int | float) -> float:\n",
    "    \"\"\"Returns the multiplication of two numbers.\"\"\"\n",
    "    return num_1 * num_2\n",
    "\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name_or_callable = \"subtraction\",\n",
    "    description =  \"Use when you need to perform subtraction operation.\",\n",
    "    args_schema = InputSchema\n",
    ")\n",
    "def subtraction(num_1: int | float, num_2: int | float) -> float:\n",
    "    \"\"\"Returns the subtraction of two numbers.\"\"\"\n",
    "    return num_1 - num_2\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name_or_callable = \"division\",\n",
    "    description = \"Use when you need to perform division operation.\",\n",
    "    args_schema = InputSchema\n",
    ")\n",
    "\n",
    "def division(num_1: int | float, num_2: int | float) -> float:\n",
    "    \"\"\"Returns the division of two numbers.\"\"\"\n",
    "    if num_2 == 0:\n",
    "        raise ValueError(\"Division by zero is not allowed.\")\n",
    "    return num_1 / num_2\n",
    "\n",
    "\n",
    "tavily_search = TavilySearch(\n",
    "    tavily_api_key = settings.tavily_api_key,\n",
    "    max_results = 2\n",
    ")\n",
    "\n",
    "available_tools: List[StructuredTool | TavilySearch] = [\n",
    "    addition, subtraction, multiplication, division, get_current_date, tavily_search\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221624f",
   "metadata": {},
   "source": [
    "### We bind this tools to our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527ceec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000234B77AFE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000234B7938AD0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=2000), kwargs={'tools': [{'type': 'function', 'function': {'name': 'addition', 'description': 'Use when you need to perform addition operation.', 'parameters': {'properties': {'num_1': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The first number (integer or float) eg. 8 or 8.5'}, 'num_2': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The second number (integer or float) eg. 8 or 8.5'}}, 'required': ['num_1', 'num_2'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'subtraction', 'description': 'Use when you need to perform subtraction operation.', 'parameters': {'properties': {'num_1': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The first number (integer or float) eg. 8 or 8.5'}, 'num_2': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The second number (integer or float) eg. 8 or 8.5'}}, 'required': ['num_1', 'num_2'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'multiplication', 'description': 'Use when you need to perform multiplication operation.', 'parameters': {'properties': {'num_1': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The first number (integer or float) eg. 8 or 8.5'}, 'num_2': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The second number (integer or float) eg. 8 or 8.5'}}, 'required': ['num_1', 'num_2'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'division', 'description': 'Use when you need to perform division operation.', 'parameters': {'properties': {'num_1': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The first number (integer or float) eg. 8 or 8.5'}, 'num_2': {'anyOf': [{'type': 'integer'}, {'type': 'number'}], 'description': 'The second number (integer or float) eg. 8 or 8.5'}}, 'required': ['num_1', 'num_2'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'date_time', 'description': 'Use when you need to get current date and time.', 'parameters': {'properties': {}, 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'tavily_search', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'Search query to look up', 'type': 'string'}, 'include_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to restrict search results to.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests information from specific websites (e.g., \"Find climate data from nasa.gov\")\\n        2. The user mentions an organization or company without specifying the domain (e.g., \"Find information about iPhones from Apple\")\\n\\n        In both cases, you should determine the appropriate domains (e.g., [\"nasa.gov\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will ONLY come from the specified domains - no other sources will be included.\\n        Default is None (no domain restriction).\\n        '}, 'exclude_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to exclude from search results.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests to avoid certain websites (e.g., \"Find information about climate change but not from twitter.com\")\\n        2. The user mentions not wanting results from specific organizations without naming the domain (e.g., \"Find phone reviews but nothing from Apple\")\\n\\n        In both cases, you should determine the appropriate domains to exclude (e.g., [\"twitter.com\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will filter out all content from the specified domains.\\n        Default is None (no domain exclusion).\\n        '}, 'search_depth': {'anyOf': [{'enum': ['basic', 'advanced'], 'type': 'string'}, {'type': 'null'}], 'default': 'basic', 'description': 'Controls search thoroughness and result comprehensiveness.\\n    \\n        Use \"basic\" for simple queries requiring quick, straightforward answers.\\n        \\n        Use \"advanced\" (default) for complex queries, specialized topics, \\n        rare information, or when in-depth analysis is needed.\\n        '}, 'include_images': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': 'Determines if the search returns relevant images along with text results.\\n   \\n        Set to True when the user explicitly requests visuals or when images would \\n        significantly enhance understanding (e.g., \"Show me what black holes look like,\" \\n        \"Find pictures of Renaissance art\").\\n        \\n        Leave as False (default) for most informational queries where text is sufficient.\\n        '}, 'time_range': {'anyOf': [{'enum': ['day', 'week', 'month', 'year'], 'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Limits results to content published within a specific timeframe.\\n        \\n        ONLY set this when the user explicitly mentions a time period \\n        (e.g., \"latest AI news,\" \"articles from last week\").\\n        \\n        For less popular or niche topics, use broader time ranges \\n        (\"month\" or \"year\") to ensure sufficient relevant results.\\n   \\n        Options: \"day\" (24h), \"week\" (7d), \"month\" (30d), \"year\" (365d).\\n        \\n        Default is None.\\n        '}, 'topic': {'anyOf': [{'enum': ['general', 'news', 'finance'], 'type': 'string'}, {'type': 'null'}], 'default': 'general', 'description': 'Specifies search category for optimized results.\\n   \\n        Use \"general\" (default) for most queries, INCLUDING those with terms like \\n        \"latest,\" \"newest,\" or \"recent\" when referring to general information.\\n\\n        Use \"finance\" for markets, investments, economic data, or financial news.\\n\\n        Use \"news\" ONLY for politics, sports, or major current events covered by \\n        mainstream media - NOT simply because a query asks for \"new\" information.\\n        '}, 'include_favicon': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': \"Determines whether to include favicon URLs for each search result.\\n        \\n        When enabled, each search result will include the website's favicon URL,\\n        which can be useful for:\\n        - Building rich UI interfaces with visual website indicators\\n        - Providing visual cues about the source's credibility or brand\\n        - Creating bookmark-like displays with recognizable site icons\\n        \\n        Set to True when creating user interfaces that benefit from visual branding\\n        or when favicon information enhances the user experience.\\n        \\n        Default is False to minimize response size and API usage.\\n        \"}, 'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or after this date.\\n        \\n        Use this parameter when you need to:\\n        - Find recent developments or updates on a topic\\n        - Exclude outdated information from search results\\n        - Focus on content within a specific timeframe\\n        - Combine with end_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-01-15\" for January 15, 2024).\\n        \\n        Examples:\\n        - \"2024-01-01\" - Results from January 1, 2024 onwards\\n        - \"2023-12-25\" - Results from December 25, 2023 onwards\\n        \\n        When combined with end_date, creates a precise date range filter.\\n        \\n        Default is None (no start date restriction).\\n        '}, 'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or before this date.\\n        \\n        Use this parameter when you need to:\\n        - Exclude content published after a certain date\\n        - Study historical information or past events\\n        - Research how topics were covered during specific time periods\\n        - Combine with start_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-03-31\" for March 31, 2024).\\n        \\n        Examples:\\n        - \"2024-03-31\" - Results up to and including March 31, 2024\\n        - \"2023-12-31\" - Results up to and including December 31, 2023\\n        \\n        When combined with start_date, creates a precise date range filter.\\n        For example: start_date=\"2024-01-01\", end_date=\"2024-03-31\" \\n        returns results from Q1 2024 only.\\n        \\n        Default is None (no end date restriction).\\n        '}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools(tools = available_tools)\n",
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4ce0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"bound\": {\n",
      "        \"name\": null,\n",
      "        \"disable_streaming\": false,\n",
      "        \"model_name\": \"llama-3.1-8b-instant\",\n",
      "        \"temperature\": 0.7,\n",
      "        \"stop\": null,\n",
      "        \"reasoning_format\": null,\n",
      "        \"reasoning_effort\": null,\n",
      "        \"model_kwargs\": {},\n",
      "        \"groq_api_key\": \"**********\",\n",
      "        \"groq_api_base\": null,\n",
      "        \"groq_proxy\": null,\n",
      "        \"request_timeout\": null,\n",
      "        \"max_retries\": 2,\n",
      "        \"streaming\": false,\n",
      "        \"n\": 1,\n",
      "        \"max_tokens\": 2000,\n",
      "        \"service_tier\": \"on_demand\",\n",
      "        \"default_headers\": null,\n",
      "        \"default_query\": null,\n",
      "        \"http_client\": null,\n",
      "        \"http_async_client\": null\n",
      "    },\n",
      "    \"kwargs\": {\n",
      "        \"tools\": [\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"addition\",\n",
      "                    \"description\": \"Use when you need to perform addition operation.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {\n",
      "                            \"num_1\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The first number (integer or float) eg. 8 or 8.5\"\n",
      "                            },\n",
      "                            \"num_2\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The second number (integer or float) eg. 8 or 8.5\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                            \"num_1\",\n",
      "                            \"num_2\"\n",
      "                        ],\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"subtraction\",\n",
      "                    \"description\": \"Use when you need to perform subtraction operation.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {\n",
      "                            \"num_1\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The first number (integer or float) eg. 8 or 8.5\"\n",
      "                            },\n",
      "                            \"num_2\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The second number (integer or float) eg. 8 or 8.5\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                            \"num_1\",\n",
      "                            \"num_2\"\n",
      "                        ],\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"multiplication\",\n",
      "                    \"description\": \"Use when you need to perform multiplication operation.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {\n",
      "                            \"num_1\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The first number (integer or float) eg. 8 or 8.5\"\n",
      "                            },\n",
      "                            \"num_2\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The second number (integer or float) eg. 8 or 8.5\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                            \"num_1\",\n",
      "                            \"num_2\"\n",
      "                        ],\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"division\",\n",
      "                    \"description\": \"Use when you need to perform division operation.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {\n",
      "                            \"num_1\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The first number (integer or float) eg. 8 or 8.5\"\n",
      "                            },\n",
      "                            \"num_2\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"integer\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"number\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"description\": \"The second number (integer or float) eg. 8 or 8.5\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                            \"num_1\",\n",
      "                            \"num_2\"\n",
      "                        ],\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"date_time\",\n",
      "                    \"description\": \"Use when you need to get current date and time.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {},\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"function\",\n",
      "                \"function\": {\n",
      "                    \"name\": \"tavily_search\",\n",
      "                    \"description\": \"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.\",\n",
      "                    \"parameters\": {\n",
      "                        \"properties\": {\n",
      "                            \"query\": {\n",
      "                                \"description\": \"Search query to look up\",\n",
      "                                \"type\": \"string\"\n",
      "                            },\n",
      "                            \"include_domains\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"items\": {\n",
      "                                            \"type\": \"string\"\n",
      "                                        },\n",
      "                                        \"type\": \"array\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": [],\n",
      "                                \"description\": \"A list of domains to restrict search results to.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests information from specific websites (e.g., \\\"Find climate data from nasa.gov\\\")\\n        2. The user mentions an organization or company without specifying the domain (e.g., \\\"Find information about iPhones from Apple\\\")\\n\\n        In both cases, you should determine the appropriate domains (e.g., [\\\"nasa.gov\\\"] or [\\\"apple.com\\\"]) and set this parameter.\\n\\n        Results will ONLY come from the specified domains - no other sources will be included.\\n        Default is None (no domain restriction).\\n        \"\n",
      "                            },\n",
      "                            \"exclude_domains\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"items\": {\n",
      "                                            \"type\": \"string\"\n",
      "                                        },\n",
      "                                        \"type\": \"array\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": [],\n",
      "                                \"description\": \"A list of domains to exclude from search results.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests to avoid certain websites (e.g., \\\"Find information about climate change but not from twitter.com\\\")\\n        2. The user mentions not wanting results from specific organizations without naming the domain (e.g., \\\"Find phone reviews but nothing from Apple\\\")\\n\\n        In both cases, you should determine the appropriate domains to exclude (e.g., [\\\"twitter.com\\\"] or [\\\"apple.com\\\"]) and set this parameter.\\n\\n        Results will filter out all content from the specified domains.\\n        Default is None (no domain exclusion).\\n        \"\n",
      "                            },\n",
      "                            \"search_depth\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"enum\": [\n",
      "                                            \"basic\",\n",
      "                                            \"advanced\"\n",
      "                                        ],\n",
      "                                        \"type\": \"string\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": \"basic\",\n",
      "                                \"description\": \"Controls search thoroughness and result comprehensiveness.\\n    \\n        Use \\\"basic\\\" for simple queries requiring quick, straightforward answers.\\n        \\n        Use \\\"advanced\\\" (default) for complex queries, specialized topics, \\n        rare information, or when in-depth analysis is needed.\\n        \"\n",
      "                            },\n",
      "                            \"include_images\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"boolean\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": false,\n",
      "                                \"description\": \"Determines if the search returns relevant images along with text results.\\n   \\n        Set to True when the user explicitly requests visuals or when images would \\n        significantly enhance understanding (e.g., \\\"Show me what black holes look like,\\\" \\n        \\\"Find pictures of Renaissance art\\\").\\n        \\n        Leave as False (default) for most informational queries where text is sufficient.\\n        \"\n",
      "                            },\n",
      "                            \"time_range\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"enum\": [\n",
      "                                            \"day\",\n",
      "                                            \"week\",\n",
      "                                            \"month\",\n",
      "                                            \"year\"\n",
      "                                        ],\n",
      "                                        \"type\": \"string\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": null,\n",
      "                                \"description\": \"Limits results to content published within a specific timeframe.\\n        \\n        ONLY set this when the user explicitly mentions a time period \\n        (e.g., \\\"latest AI news,\\\" \\\"articles from last week\\\").\\n        \\n        For less popular or niche topics, use broader time ranges \\n        (\\\"month\\\" or \\\"year\\\") to ensure sufficient relevant results.\\n   \\n        Options: \\\"day\\\" (24h), \\\"week\\\" (7d), \\\"month\\\" (30d), \\\"year\\\" (365d).\\n        \\n        Default is None.\\n        \"\n",
      "                            },\n",
      "                            \"topic\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"enum\": [\n",
      "                                            \"general\",\n",
      "                                            \"news\",\n",
      "                                            \"finance\"\n",
      "                                        ],\n",
      "                                        \"type\": \"string\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": \"general\",\n",
      "                                \"description\": \"Specifies search category for optimized results.\\n   \\n        Use \\\"general\\\" (default) for most queries, INCLUDING those with terms like \\n        \\\"latest,\\\" \\\"newest,\\\" or \\\"recent\\\" when referring to general information.\\n\\n        Use \\\"finance\\\" for markets, investments, economic data, or financial news.\\n\\n        Use \\\"news\\\" ONLY for politics, sports, or major current events covered by \\n        mainstream media - NOT simply because a query asks for \\\"new\\\" information.\\n        \"\n",
      "                            },\n",
      "                            \"include_favicon\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"boolean\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": false,\n",
      "                                \"description\": \"Determines whether to include favicon URLs for each search result.\\n        \\n        When enabled, each search result will include the website's favicon URL,\\n        which can be useful for:\\n        - Building rich UI interfaces with visual website indicators\\n        - Providing visual cues about the source's credibility or brand\\n        - Creating bookmark-like displays with recognizable site icons\\n        \\n        Set to True when creating user interfaces that benefit from visual branding\\n        or when favicon information enhances the user experience.\\n        \\n        Default is False to minimize response size and API usage.\\n        \"\n",
      "                            },\n",
      "                            \"start_date\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"string\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": null,\n",
      "                                \"description\": \"Filters search results to include only content published on or after this date.\\n        \\n        Use this parameter when you need to:\\n        - Find recent developments or updates on a topic\\n        - Exclude outdated information from search results\\n        - Focus on content within a specific timeframe\\n        - Combine with end_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \\\"2024-01-15\\\" for January 15, 2024).\\n        \\n        Examples:\\n        - \\\"2024-01-01\\\" - Results from January 1, 2024 onwards\\n        - \\\"2023-12-25\\\" - Results from December 25, 2023 onwards\\n        \\n        When combined with end_date, creates a precise date range filter.\\n        \\n        Default is None (no start date restriction).\\n        \"\n",
      "                            },\n",
      "                            \"end_date\": {\n",
      "                                \"anyOf\": [\n",
      "                                    {\n",
      "                                        \"type\": \"string\"\n",
      "                                    },\n",
      "                                    {\n",
      "                                        \"type\": \"null\"\n",
      "                                    }\n",
      "                                ],\n",
      "                                \"default\": null,\n",
      "                                \"description\": \"Filters search results to include only content published on or before this date.\\n        \\n        Use this parameter when you need to:\\n        - Exclude content published after a certain date\\n        - Study historical information or past events\\n        - Research how topics were covered during specific time periods\\n        - Combine with start_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \\\"2024-03-31\\\" for March 31, 2024).\\n        \\n        Examples:\\n        - \\\"2024-03-31\\\" - Results up to and including March 31, 2024\\n        - \\\"2023-12-31\\\" - Results up to and including December 31, 2023\\n        \\n        When combined with start_date, creates a precise date range filter.\\n        For example: start_date=\\\"2024-01-01\\\", end_date=\\\"2024-03-31\\\" \\n        returns results from Q1 2024 only.\\n        \\n        Default is None (no end date restriction).\\n        \"\n",
      "                            }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                            \"query\"\n",
      "                        ],\n",
      "                        \"type\": \"object\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"config\": {},\n",
      "    \"config_factories\": [],\n",
      "    \"custom_input_type\": null,\n",
      "    \"custom_output_type\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(llm_with_tools.model_dump_json(indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a789301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'tp21n611n', 'function': {'arguments': '{\"num_1\":90.78,\"num_2\":465.43}', 'name': 'addition'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2356, 'total_tokens': 2383, 'completion_time': 0.036, 'prompt_time': 0.30879163, 'queue_time': 0.052429521000000034, 'total_time': 0.34479163}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3f242eca-7d10-44e0-ad88-85db247b7239-0', tool_calls=[{'name': 'addition', 'args': {'num_1': 90.78, 'num_2': 465.43}, 'id': 'tp21n611n', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2356, 'output_tokens': 27, 'total_tokens': 2383})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"what is the sum of 90.78 and 465.43\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31b354c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31444d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'addition', 'args': {'num_1': 90.78, 'num_2': 465.43}, 'id': 'tp21n611n', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edb6f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\"what is the product of 90.78 and 465.43\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a254d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiplication', 'args': {'num_1': 90.78, 'num_2': 465.43}, 'id': 'vbqqrqknr', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5932ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'at1rf4wa8', 'function': {'arguments': '{\"query\":\"LangGraph\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 2347, 'total_tokens': 2370, 'completion_time': 0.051112056, 'prompt_time': 0.191217512, 'queue_time': 0.052991334, 'total_time': 0.242329568}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--b515eb1b-0fbc-48ff-9619-1b8555a5f408-0' tool_calls=[{'name': 'tavily_search', 'args': {'query': 'LangGraph', 'topic': 'general'}, 'id': 'at1rf4wa8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2347, 'output_tokens': 23, 'total_tokens': 2370}\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"what is LangGraph?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25dd480f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search',\n",
       "  'args': {'query': 'LangGraph', 'topic': 'general'},\n",
       "  'id': 'at1rf4wa8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccff3f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': '39ec2e5zs', 'function': {'arguments': '{\"query\":\"current temperature in chennai\",\"topic\":\"news\"}', 'name': 'tavily_search'}, 'type': 'function'}, {'id': '81v8en874', 'function': {'arguments': '{}', 'name': 'date_time'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 2351, 'total_tokens': 2385, 'completion_time': 0.082211284, 'prompt_time': 0.189937938, 'queue_time': 0.052965758, 'total_time': 0.272149222}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--938e800b-643d-409b-905b-0b8a51f56f68-0' tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current temperature in chennai', 'topic': 'news'}, 'id': '39ec2e5zs', 'type': 'tool_call'}, {'name': 'date_time', 'args': {}, 'id': '81v8en874', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2351, 'output_tokens': 34, 'total_tokens': 2385}\n",
      "[{'name': 'tavily_search', 'args': {'query': 'current temperature in chennai', 'topic': 'news'}, 'id': '39ec2e5zs', 'type': 'tool_call'}, {'name': 'date_time', 'args': {}, 'id': '81v8en874', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"what is the current temperature in chennai?\")\n",
    "print(response, end = \"\\n\")\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee038f3",
   "metadata": {},
   "source": [
    "## Utilize LangGraph's ToolNode to Execute the tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b44b48b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tools(tags=None, recurse=True, explode_args=False, func_accepts_config=True, func_accepts={'store': ('__pregel_store', None)}, tools_by_name={'addition': StructuredTool(name='addition', description='Use when you need to perform addition operation.', args_schema=<class '__main__.InputSchema'>, func=<function addition at 0x00000210CF7E1C60>), 'subtraction': StructuredTool(name='subtraction', description='Use when you need to perform subtraction operation.', args_schema=<class '__main__.InputSchema'>, func=<function subtraction at 0x00000210CF7E0220>), 'multiplication': StructuredTool(name='multiplication', description='Use when you need to perform multiplication operation.', args_schema=<class '__main__.InputSchema'>, func=<function multiplication at 0x00000210CF7E0400>), 'division': StructuredTool(name='division', description='Use when you need to perform division operation.', args_schema=<class '__main__.InputSchema'>, func=<function division at 0x00000210CF7E22A0>), 'date_time': StructuredTool(name='date_time', description='Use when you need to get current date and time.', args_schema=<class 'langchain_core.utils.pydantic.date_time'>, func=<function get_current_date at 0x00000210CF7E1DA0>), 'tavily_search': TavilySearch(max_results=2, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'), api_base_url=None))}, tool_to_state_args={'addition': {}, 'subtraction': {}, 'multiplication': {}, 'division': {}, 'date_time': {}, 'tavily_search': {}}, tool_to_store_arg={'addition': None, 'subtraction': None, 'multiplication': None, 'division': None, 'date_time': None, 'tavily_search': None}, handle_tool_errors=True, messages_key='messages')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools = available_tools)\n",
    "tool_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbcec988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.prebuilt.tool_node.ToolNode'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tool_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d91ce5",
   "metadata": {},
   "source": [
    "### Let's test the ToolNode with a sample LLM Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d854abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ret7ca176', 'function': {'arguments': '{\"num_1\":908.657,\"num_2\":87.365}', 'name': 'addition'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2356, 'total_tokens': 2383, 'completion_time': 0.045458666, 'prompt_time': 0.546220925, 'queue_time': 0.05245337400000005, 'total_time': 0.591679591}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0ff61786-35d3-4869-abb0-5aa0817f4ddd-0', tool_calls=[{'name': 'addition', 'args': {'num_1': 908.657, 'num_2': 87.365}, 'id': 'ret7ca176', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2356, 'output_tokens': 27, 'total_tokens': 2383})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"what is the sum of 908.657 and 87.365\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "996d4ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the sum of 908.657 and 87.365', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ret7ca176', 'function': {'arguments': '{\"num_1\":908.657,\"num_2\":87.365}', 'name': 'addition'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2356, 'total_tokens': 2383, 'completion_time': 0.045458666, 'prompt_time': 0.546220925, 'queue_time': 0.05245337400000005, 'total_time': 0.591679591}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0ff61786-35d3-4869-abb0-5aa0817f4ddd-0', tool_calls=[{'name': 'addition', 'args': {'num_1': 908.657, 'num_2': 87.365}, 'id': 'ret7ca176', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2356, 'output_tokens': 27, 'total_tokens': 2383})]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_messages_state: BasicChatBotState = BasicChatBotState(\n",
    "    messages = [\n",
    "        HumanMessage(content = \"what is the sum of 908.657 and 87.365\"),\n",
    "        AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ret7ca176', 'function': {'arguments': '{\"num_1\":908.657,\"num_2\":87.365}', 'name': 'addition'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2356, 'total_tokens': 2383, 'completion_time': 0.045458666, 'prompt_time': 0.546220925, 'queue_time': 0.05245337400000005, 'total_time': 0.591679591}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0ff61786-35d3-4869-abb0-5aa0817f4ddd-0', tool_calls=[{'name': 'addition', 'args': {'num_1': 908.657, 'num_2': 87.365}, 'id': 'ret7ca176', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2356, 'output_tokens': 27, 'total_tokens': 2383})\n",
    "    ]\n",
    ")\n",
    "sample_messages_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cec7d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_result = tool_node.invoke(sample_messages_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f42155dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [ToolMessage(content='996.022', name='addition', tool_call_id='ret7ca176')]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3a629",
   "metadata": {},
   "source": [
    "#### Now we get the final result from the ToolNode. The result we get it as ToolMessage. The content hold the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1d20",
   "metadata": {},
   "source": [
    "## Now Create a Graph with a ToolNode, This ensure our ChatBot has some tools that it can use whenever needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d31574",
   "metadata": {},
   "source": [
    "### Copy and paste the same code from simple chatbot implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53c49610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000210D19D6350>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, add_messages\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Bind all the custom tools to the LLM.\n",
    "llm_with_tools = llm.bind_tools(tools = available_tools)\n",
    "# Create a ToolNode to execute appropriate tools.\n",
    "tool_node: ToolNode = ToolNode(\n",
    "    tools = available_tools,\n",
    "    messages_key = \"messages\"\n",
    ")\n",
    "\n",
    "CHATBOT = \"chatbot\"\n",
    "TOOLS = \"tools\"\n",
    "\n",
    "\n",
    "class BasicChatBotState(TypedDict):\n",
    "    \"Represents the state of basic ChatBot\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def chatbot_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Execute tools and return the results.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke(state[\"messages\"])]\n",
    "    }\n",
    "\n",
    "def tool_executor_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    tool_response: List[ToolMessage] = tool_node.invoke(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": tool_response\n",
    "    }    \n",
    "\n",
    "def tool_call_required(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Check if the last message requires tool calls.\"\"\"\n",
    "    last_ai_message: AIMessage = state[\"messages\"][-1]\n",
    "    if last_ai_message.tool_calls:\n",
    "        return TOOLS\n",
    "    return END \n",
    "\n",
    "\n",
    "# Build the Graph Workflow.\n",
    "graph_builder = StateGraph(BasicChatBotState)\n",
    "graph_builder.add_node(CHATBOT, chatbot_node)\n",
    "graph_builder.add_node(TOOLS, tool_executor_node)\n",
    "graph_builder.set_entry_point(CHATBOT)\n",
    "graph_builder.add_edge(TOOLS, CHATBOT)\n",
    "graph_builder.add_conditional_edges(\n",
    "    CHATBOT,\n",
    "    tool_call_required,\n",
    "    path_map = {\n",
    "        TOOLS: TOOLS,\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(CHATBOT, END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1966e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"Hi, this is Nagarjun, Could you tell me what is the product of 768.0909 and 8.454\")\n",
    "        ]\n",
    "    )\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e62fd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, this is Nagarjun, Could you tell me what is the product of 768.0909 and 8.454', additional_kwargs={}, response_metadata={}, id='e3c51707-2c48-4954-9d48-741102896ec5'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'jvtjtc2pg', 'function': {'arguments': '{\"num_1\":768.0909,\"num_2\":8.454}', 'name': 'multiplication'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 2368, 'total_tokens': 2397, 'completion_time': 0.038792865, 'prompt_time': 0.156068998, 'queue_time': 0.053239958000000004, 'total_time': 0.194861863}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d6a652d8-2d41-4c41-9d78-1ae83be19803-0', tool_calls=[{'name': 'multiplication', 'args': {'num_1': 768.0909, 'num_2': 8.454}, 'id': 'jvtjtc2pg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2368, 'output_tokens': 29, 'total_tokens': 2397}),\n",
       " ToolMessage(content='6493.440468600001', name='multiplication', id='d956d58f-c601-43ef-a5e2-0459fc220202', tool_call_id='jvtjtc2pg'),\n",
       " AIMessage(content='The product of 768.0909 and 8.454 is 6493.440468600001.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 2414, 'total_tokens': 2438, 'completion_time': 0.089413343, 'prompt_time': 0.025355347, 'queue_time': 0.054654883, 'total_time': 0.11476869}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce4cd540-57fb-4b46-b75e-7fcc445919db-0', usage_metadata={'input_tokens': 2414, 'output_tokens': 24, 'total_tokens': 2438})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b18a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, this is Nagarjun.', additional_kwargs={}, response_metadata={}, id='a9fd042a-5f38-4e13-9f02-1afcccda0558'),\n",
       "  AIMessage(content='Hello Nagarjun. How are you today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 2349, 'total_tokens': 2368, 'completion_time': 0.035364857, 'prompt_time': 0.325201939, 'queue_time': 0.05240837100000001, 'total_time': 0.360566796}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7cecb5e5-c0c9-4252-8e9f-889fb4b03d9d-0', usage_metadata={'input_tokens': 2349, 'output_tokens': 19, 'total_tokens': 2368})]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"Hi, this is Nagarjun.\")\n",
    "        ]\n",
    "    )\n",
    ") \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbbf6bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 3.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}, id='8461297c-24b2-4ee4-8bb5-405de3b3fc0a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '9svdw4w0a', 'function': {'arguments': '{\"query\":\"LangGraph\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 2347, 'total_tokens': 2370, 'completion_time': 0.112776415, 'prompt_time': 0.333663968, 'queue_time': 0.05502443000000001, 'total_time': 0.446440383}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--23b7f1f9-13c7-434c-bbb4-0986bc154ea8-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'LangGraph', 'topic': 'general'}, 'id': '9svdw4w0a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2347, 'output_tokens': 23, 'total_tokens': 2370}),\n",
       "  ToolMessage(content='{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://github.com/langchain-ai/langgraph\", \"title\": \"langchain-ai/langgraph: Build resilient language agents as graphs.\", \"content\": \"GitHub - langchain-ai/langgraph: Build resilient language agents as graphs. langchain-ai.github.io/langgraph/ Trusted by companies shaping the future of agents – including Klarna, Replit, Elastic, and more – LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials. LangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. *   LangGraph Platform — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. langchain-ai.github.io/langgraph/\", \"score\": 0.9057319, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph - LangChain\", \"content\": \"Design agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.” LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\", \"score\": 0.81595254, \"raw_content\": null}], \"response_time\": 1.05}', name='tavily_search', id='9cefb9b2-0c14-4286-bc80-1b07b36c5f9f', tool_call_id='9svdw4w0a'),\n",
       "  AIMessage(content='LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. It provides a customizable architecture, long-term memory, and complex task handling, and can be used standalone or integrated with LangChain products. It is designed to help developers build agent-driven user experiences and deploy and scale their applications with infrastructure built for agents.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 2867, 'total_tokens': 2941, 'completion_time': 0.098666667, 'prompt_time': 0.748646246, 'queue_time': 0.05287324500000001, 'total_time': 0.847312913}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--223eb59a-75cf-4fea-bc1a-28723cd5719a-0', usage_metadata={'input_tokens': 2867, 'output_tokens': 74, 'total_tokens': 2941})]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"What is LangGraph?\")\n",
    "        ]\n",
    "    )\n",
    ") \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e881c0",
   "metadata": {},
   "source": [
    "### ChatBot With Memory/CheckPointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8476f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXlcVNXfx8+dnVlhFnaQRQQBFRSjyBXM3QRzr1+av9K0RUqzrEzTFn20tEwlTCvJFBX3JXNJVAwVEBQQQZF9h2FmmGH2ef6YHuLBAUHnzj3DPe8Xf9y55845n5n5cO73nhUzmUwAgSAaCtECEAiAjIiABWREBBQgIyKgABkRAQXIiAgooBEtADq0akNDpValMKgUeoPepNPaQfMW04FCY2BsHo3No7h4OxAt50nAUDuiGVWLviizpThX2VSjcXRmsHlUNo/GF9J0Gjv4fugsirRGq1LoaQys9K7KL5TrN5DjP5BLtK4egIwITCbTtRONNSWtEi+WXyjHM4BNtKKnQqs2Fue2lN9rrbzfGjVF1G8wj2hF3YLsRrx7XX5hf13UFNHgaCeitVgZhVR37USjSqEf+x9XDh/2GIzURrx8uJ5KB89PkRAtBEeaajVHt1WNmeviHQR1TU9eI/51sE7owhg0wpFoIbbgWELlsxNFLt4sooV0CkmNeCKxyiuQHTaSFC40c2xHZdBQfmAEpCEjGdsRr51ocPd3IJULAQBTF3tkXZQ2VGmIFmIZ0hmx6JYCADAkprc9mnSHOSu8Lx+uNxlhvAeSzoipKfXho8noQjN+A7hXjzUQrcIC5DLirUvSoAi+A5dKtBDCCBvpWHSrRSnXEy2kI+QyYkme8rkpQqJVEMyIaeLs1GaiVXSEREYsyVfS6BQqlUQf2SLeQZzcNBnRKjpCol/l4R2l7wCOjQv96KOPjh079gRvfOGFFyorK3FQBBgsisSTWXm/FY/MnxgSGbGpTutvcyPm5+c/wbuqq6ulUikOcv6hXzi34r4Kv/yfALIYUas2NlRqHLh4dbmmpaUtWrRo2LBhsbGxq1evbmhoAABERERUVVWtW7du1KhRAICWlpaEhIR58+aZL9u8ebNarTa/PSYmZt++fW+88UZERERqauqUKVMAAFOnTl22bBkeajkCen0FZA2KJnLQVKtJ+rIEp8zv3r07ZMiQnTt3VldXp6WlzZ49+6233jKZTGq1esiQIUePHjVftnPnzsjIyHPnzt28efPixYsTJkz47rvvzEnjxo2bMWPGxo0b09PTdTrdlStXhgwZUlFRgZPg2tLW/d+U4ZT5kwH7oAxroZTpOQK8Pmx2djaLxVqwYAGFQnF1dQ0ODr5///6jl73yyisxMTG+vr7mlzk5OdeuXXv33XcBABiGCQSC5cuX46SwAxwBTSmDqwWHLEY0GgHDAa84JCwsTK1Wx8fHR0ZGjhgxwsvLKyIi4tHL6HT633//vXr16sLCQr1eDwAQCv9tSwoODsZJ3qNQaBiDBVdUBpca/ODwqbJ6HU6ZBwUFff/99xKJZOvWrXFxcUuWLMnJyXn0sq1btyYmJsbFxR09ejQjI+O1115rn8pgMHCS9yjKZj2VhtmsuO5AFiOy+TQVnt0JUVFRq1atOnHixJo1a2QyWXx8vLnOa8NkMqWkpMyaNSsuLs7V1RUAoFAo8NPTNUq5HrahsmQxogOHKvZg6nVGPDLPzMy8du0aAEAikUyePHnZsmUKhaK6urr9NTqdrrW11dnZ2fxSq9VevnwZDzHdQaMyOnsxiSrdImQxIgDAgUstvqPEI+ecnJwVK1YcPnxYKpXm5ubu379fIpG4ubkxmUxnZ+f09PSMjAwKheLj43P8+PGKiorm5ua1a9eGhYXJ5XKl0oIkHx8fAMC5c+dyc3PxEFyYpXDpA9cgWRIZ0TeU8zAXFyO+8sorcXFxmzZteuGFFxYuXMjhcBITE2k0GgBgwYIFN2/eXLZsWWtr61dffcVisaZPnx4bG/vMM8+8/fbbLBZrzJgxVVVVHTL09PScMmVKQkLC1q1b8RBckq/yDbF1237XkGiEtlZjPLWrOm6JB9FCCKbsnqr4Tsuo6c5EC/l/kKhGZDApzp7MrIs4dp3ZBdeON4Q8JyBaRUfgenTCm6jJom3LH3Q2c9RoNEZHR1tM0mq1dDodwyw0efj5+e3evdvaSv8hOzs7Pj6+p5L69euXmJho8V2FWQonF4bEA64nFXLdms3kXG42Gk3hoyx7sbMmFY1Gw2Ra/vEwDONycVxT4QkkUSgUDsdyCHhqV9XwOAlfSLeqRitAOiMCAE7vrg6M4NnXihxWAeYPTqIYsY2JC9z+PtlYV64mWohNSU2pF7kx4HQhSWvEf/o5vqt4dpLI3le66SapKfXO3sz+Q/lEC+kUMtaI5sBuerzXzT+leenQDZq3LiaT6diOSr6QBrMLyVsjtvH3qYaHeaqoySKfYLgaeK1CxrmmvHT56JnO3oGwV/xkNyIAoLFKc+1kI9OB4hHg4BvCYfPsvkmrvkJTeleZeUE6cLhj5AQhhQLXQBuLICP+Q+WD1ns3FQ/zlE4udKELgyOgcfg0joBqMBCtrBtgmEnRpFfKDSajqTCrhcWh9B3EHTjcEbZBh12AjNiRmpLW+kqtUqZXyvUUCqZSWNOJra2txcXFISEhVswTAMB1ogET4PCpPCeau78Dzwm6ZsLHgoxoUx48eLBy5coDBw4QLQQ67KbqRvRukBERUICMiIACZEQEFCAjIqAAGREBBciICChARkRAATIiAgqQERFQgIyIgAJkRAQUICMioAAZEQEFyIgIKEBGREABMiICCpAREVCAjIiAAmREBBQgIyKgABkRAQXIiAgoQEa0KRiGte1wgWgPMqJNMZlMdXV1RKuAEWREBBQgIyKgABkRAQXIiAgoQEZEQAEyIgIKkBERUICMiIACZEQEFCAjIqAAGREBBciICChARkRAATIiAgqQERFQgDb8sQWzZ89WqVQAAK1W29jY6ObmZt6C/uzZs0RLgwVUI9qCqVOn1tTUVFVVNTQ0mEymqqqqqqoqHo9HtC6IQEa0BbNnz/b29m5/BsOwYcOGEacIOpARbQGGYdOmTaNSqW1n+vTpM2vWLEJFwQUyoo2YOXOml5eX+RjDsJEjR5ojRYQZZEQbQaPRZs+ezWQyAQCenp7Tp08nWhFcICPajmnTpnl6egIAoqKiUHXYARrRAghGpzVKa7QtchvtUz8l5vVzxnOjnplVnKu0QXEUCnByZgjEdrCPOKnbEdNPNxbdaqEzKTwh3aDrhd8D15FWXqgUiOmDo528A9lEy+kK8hoxNaUewyjhMSKiheCOTmM8l1Q5bKrIoy+8XiRpjJh2vIFCJYULAQB0JmXi616XDjXUV2qI1tIpZDSiollXW6oOG00KF7bx3BRJ5nkp0So6hYxGbKrWYlTSfXCBmFFWoCJaRaeQ7vcAAMileqELk2gVtobBovJEdLXKRu0DPYWMRgRGoNMaiRZBAIomHYZhRKuwDCmNiIAPZEQEFCAjIqAAGREBBciICChARkRAATIiAgqQERFQgIyIgAJkRAQUICMioAAZ8amYMWvCT7u2PU0Oq9esWLZ8sfUU2SvIiARw5OiBrzesfpocHj58MHvuZOspIh5kRAK4dy//aXMofNocYIPss/i6icFgOHho7697EgEAwf0HzJ+3aMCAMHMSjUY/fCQ54cctDAYjNDRs5UdrBXyBudI6fuJQ1q2bNTVVPn38Jk6MnfridABA/PsLc3KyAAB//nnqx4TfzPPtMzKvJyfvyc3L8ffv9+47K/oFBJkzT0tL/XVPYmnZQ4HAsW/fwKXvfOji4vrzLwl7kn4CAIyOiThz6iqLxSL0u7EOqEbsFok7tx47dnDt55s+/fhLicTlw5XvlJWVmJNSL59XKls2rN/6wfLPcnOzf/55h/n8tu3f3Lz599J3P1z/9fcTJ8Z+9/2G9OtpAIAt3yb27x86duykvy5kmA1XWvbw6LEDc+e+9tWXW4xG46er3jfPaMvIvP7Zmg/Gjp10YP/p1avW19ZWb/l+PQDgtflvzp71qouL618XMnqHC1GN2C0ULYoDB3+LX/rR0IhnAQCRkc+rVMrGpgZvbx8AAJvN+c8r/zVfmXYt9fadW+bjVau+VqmUbq7uAIDwsIg//jh+4+a1ZyOffzR/qbQp/t2PxGIJAODV/7yx8uOlOTlZYWFDdv+8Y8Tw6OkvzQUACASOSxa/v/yDJQX38oMCg237BdgCZMTHU15WAgAICgoxv6TRaGs/39iWOiA0rO1YwHfUav5vppzJdPjw/us30srLS80n3Nw8LObv7xdgdiEAIDRkEACgqroiLGxIcXHRyBExbZcF9gsGABQU5CEjkpQWZQsAgMW0fBOk0f79DtsG4huNxo8+XqrTad94/e2wsAgel/fO0v92lj+Hw207ZrPZAAC5XNbS0qLRaJjtCjUnqVS2WCLC9qAY8fFw2JyeOqCwqKCgIG/xm+8NHzaax+UBAFpaFJ1d3KpubTs2m57PF5iDP3W7JKVKCQAQCcVP8VHgBRnx8fj4+NNotJzbWeaXJpPpo4+Xnj17sou3yGTNAACJ2Nn8sqSkuKSkuLOLy8oeqtVq87G5ZcfTw5tGowX265+Xd7vtMvOxn3+AlT4WXCAjPh4Oh/PCmInHjh0888fxW9kZW3/YmJl5vX//0C7e4tPHj0ajJR9IkivkZWUlW3/YODTi2ZraanOqh4fX3bu5WbduSqVNAAAWy2HTN+vkCnlzs3Tv77udnV3MbUNxsbOupl1KSdknV8hvZWds3/Ht4PChAX0DAQCent6NjQ1Xr14yGCCdHtpTkBG7xdJ3PwwLi/jm2y/fX/bmnTvZa9dsND8yd4aLi+snH3+Rf/fO1Njojz997/X/vvXii9Pv3s2d99p0AMCUSdMwDPtgxVsPiot0el1oyCBvb98ZM8fPmDXBYDB8se5bc6w5duyk/y5YknwwaWps9Ib/WTNwQPhnq7425/9s5LABoWGrVi/XarW2+g7whYyLMN25Kqst10ZOlBAtxNbs21A8b5UP0wHG2gdGTQgSgoyIgAJkRAQUICMioAAZEQEFyIgIKEBGREABMiICCpAREVCAjIiAAmREBBQgIyKgABkRAQVkNCKdQWGyyPjBRW5MCrUb1xEBGX8PoRu94j68W9/ghKxRq5Lr6QxIf3FIZeGKsxeLwcQ0rb1kbHM3qStr7RvO7caFxEBGIwIAhsWKz++tIlqF7agqVhVclz03Ed7tB8k4QttMY7Xm0JaKiPESgZjOFdB75deAYaCpRqNo0j7IUcz+wItCgXTbKVIbEQCgVRtv/tl491YtFWNRTLaY4m00mXQ6HZPBwCl/pUqFYRiVSqVQKBQKRezBwjDgHcgeNMIRpxKtBakn2FPpJnFgk6E67fVFi2xT4oMHD1au/PTAgQM45b9y5cqzZ89iGObk5MTlcpkFTHd39376foNGwL4EI3lrxD179kyaNInD4dhyHSOFQpGZmTlq1Cic8i8oKIiPj29oaGh/0mg0urm5nTp1CqdCrQJJH1ZSUlKkUqlIJLLxalo8Hg8/FwIAgoKC+vfv3+Ekh8OB3IVkNOLFixcBAM8///zSpUttX3p9ff327dtxLWLu3LlOTk5tLykUypUrV3At0SqQy4jr168vLi4GALi6uhIiQC6XX7p0Cdcihg4d6u/vb464jEajn5/fsWPHcC3RKlDXrFlDtAZbcP/+faFQyOFwJk2aRKAMOp3u6enp49PVKhFPD5vNvnHjhkaj8fT0TElJOXDgQFpa2vDhw3Et9CkhxcPKypUrY2JixowZQ7QQ2/Hyyy/X1taeP3/e/DIlJeXIkSO//fYb0bo6x9SrUSgU5eXlZ8+eJVrIP9TV1W3bto2QovPz84cMGZKbm0tI6Y+lN8eI69ata2ho8PT0HDt2LNFa/sEGMWJn9O/fPyMjY8OGDYcOHSJEQNf0WiOmpKQMGDAA72ispzg7Oy9ZsoRAAXv27CkqKvr8888J1GCRXhgjJiYmLly4UKvVMnDrSbN3jh8/vnfv3qSkJHi+ot5WI3722WeOjo4AAHi+4vbYoB2xO7z44otffvnlyJEjs7OzidbyfxAdpFqNS5cumUym+vp6ooV0xf3792fMmEG0in9ZsGDB3r17iVZh6j0PKy+//LJ5lVWxGOq1zgmPETuwa9eu6urqTz/9lGgh9h8jVlRUODs7FxcXBwUFEa3FXjlz5szOnTuTkpI4HA5RGuy4RtTr9W+88YZarWYwGPbiQkhixA5MmDBh8+bNEyZMuHnzJlEa7NWIJpMpLS1t8eLFffv2JVpLDyCwHbFr+vTpc/ny5V27dv3666+ECLA/IxqNxvfee89kMo0cOXLw4MFEy+kZsMWIHUhISJDJZCtWrLB90fYXI65evTomJmbEiBFEC+m1XLhwYcuWLUlJSeaGMBtB9GN7D/jll1+IlvC0ENjX3CMqKyujo6OvXr1qsxLt5tY8fvz40NCuNnuyC6CNETvg7u5+4cKF5OTkn376yTYl2sGtOSsra/DgwWq1uhdsko33nBWrs2PHjsLCws2bN+NdENQ1olKpHDduHJ/PBwD0AhfaYM6K1Vm8eHFcXNy4cePq6urwLclmQUBPUSgUhYWFkHfZ9RR7iRE7UF9fP378+OzsbPyKgLRGPHz4cFZWVkBAAORddj2FxWLdunWLaBU9RiwWnzlzZtu2bZWVlTgVAekE+6KiIp1OR7QK68Pj8bZv397a2ophmN0FG1lZWe7u7jhlDmmN+Oabb06ePJloFbhAp9MdHBySk5Orq6uJ1tIDCgoKAgMDzSNL8ABSIwoEAgI74G3AvHnz4uPjiVbRA+7evfvo1H0rAqkRf/zxx5MnTxKtAl+Sk5MBAOXl5UQL6Rb5+fnBwcH45Q+pEWUymVKpJFqFLUhNTc3MzCRaxePBu0aEtEFbJpPRaLTefXdu44svvoBhaGrXREREZGRk4Jc/pDVir48R22N2YXp6OtFCOiU/Px/X6hBeI5IhRuxARUXF2bNniVZhGbzvy/AakTwxYhvTp0+Xy+VEq7AM3k8q8Bpx0aJFvbUdsQtmzJgBANi3bx/RQjpC3hqRVDFiB0QiEVSrghiNxqKiosDAQFxLgdSIJIwR2xg7dixUK6XY4L4MrxFJGCO2JyIiwrxqBdFCgG3uy/AakZwxYgfi4uL27t1LtAobGRHS0TcCgYBoCcQTHh7u4uJCtAqQn58/Z84cvEuBtEYkc4zYHvOwq7i4OKIE6PX6hw8fBgQE4F0QpEYkeYzYgYSEhKSkpPZnbLb0qG2eVFBfs92g1Wq1Wi2VSnVwcJg4cWJtbe24ceO++uorvMtNTk4uLS21wZR7FCPaBwwGg8FgDBs2zNHRsa6uDsOwvLy8pqYmoVCIa7n5+flDhw7FtQgzkN6aUYxoEZFIVFNTYz5uamqywU4+tnlkhteIKEZ8lJdeeqn93CWlUnnu3DlcS9RqteXl5f7+/riWYgbSW/OiRYtoNEi1EUJcXFxpaal5SzPzGQqFUlpaWlxc7Ofnh1OhNntSgbdGJHNfs0WOHDkSFxfn4+NjXhjJaDQCAGpra3G9O9vsvgxvjfjjjz96eHigzpX2rFq1CgBw+/btK1euXLlypbGxUSZVpV64Me3Fl3Eq8V5eWXh4uEKqf+IcTCbAF3bLY3A130RHR8tksjZJGIaZTCZXV9fTp08TLQ0uMs413b4qNWJ6vcbkgNv8aL1eT6XRnmYCqZMbs7JI1XcQJ3KiiC+kd3ElXDViVFTU6dOn28IgcyQ0ZcoUQkVBxx+/1nCF9AkLvLmOXf20kKDXGZvrtAe/q5j2loeTc6d7jsAVI86ZM6fDWgKenp426Oi0I878UuPkyhw0QmQXLgQA0OgUsQdr5vu+R7ZVyps6Xb0DLiOGhIS0XwQRw7Dx48fbdN1SuCnJVzIcqMHPOnXjWugYPcst/XRTZ6lwGREA8Oqrr7YtvOTp6Tlz5kyiFUFEXbmGzoTuJ+smTi7M+9mKzlKh+1TBwcEDBw40H0+YMMHJyS7/+3FCozKI3ZhEq3hCqDTMO5DTXK+1mAqdEQEA8+fPF4lErq6uqDrsgFJu0NvzGmlNtdrOlnF62qfmqgcqWYNeqdCr5AajAej1xqfMEAAAgGhY4GIOh5NxRgNA7dNnx3SgYABj86lsPlXkzpS422ul0ot5QiOW3lUWZrUU5yqdXB1MJoxKp1LoVAqVaq1WydCBowAACiv1NreoMKPBYKjUG7RqnVqmUxv8B3KCIngufexshcJeTI+NWP2w9fKRRjqbgdGY/s850ehUfIThiLZV39igTD0qdWCD4bEiRwmMG+qSjZ4Z8fy++qpitchXyHGy47qE4UATegkAAPI6ZcrWqv7P8KImi4gWRXa6+7Ci1xl/WVuqNjC9B7vbtQvbw3fm+D/nVVdDObINr6WhEd2kW0Y06E2JK4vdgl24ol44IsbRg08X8Pdvso8FM3srjzei0WjaseJBcIwvk2MffUpPAFfE5nsIf/2ilGgh5OXxRtz7dVlAlIdNxBAJ25El9HI8tcueFljvTTzGiJdSGhy9HJkcUjxX8py5OsDMTm0mWggZ6cqIjVWah7lKnoRrQz0E4+guuHq0AaoxmiShKyNePtoo9sV3tiKEuPZzunK0kWgVpKNTI9aUtOoNFJ6EbVs93SX7zvnlqyJblFKr5yz2caws1mhaDVbP2U6JnTZmTxLum+V2asT7OUqM2msfkx8DRinJUxEtwjp8vvaj02eOEa3i8XRqxAe3lTxnSKtDvGELOUXZLUSrsA737uUTLaFbWO7ik9ZpHXh0/B6WS8pu//nXT+UV+VyOU//AYWNHv85icQAAaekHz6XuXrxgx579K2vrit1c+o6ImjN08D9z+U7+sTUj5zSTwQ4fOM5Z7I2TNgAA35ldnQfpuuo9YnRMBABg46Z1OxI2nzh2CQCQlpb6657E0rKHAoFj376BS9/50MXF1XxxF0ltpF9PS07eU3AvTygUh4YOWvj6OyKRdbaPtVwjtjTr1a1WGdBlgYbG8h9/eUen07y98Kd5czdU1xbt2L3YYNADAKg0emur4uipTTNjP964Nn1gaPSBo19Im2sAANdupFy7cWjapA+WLvpZ5OR+7q9dOMkzT1FokeqU8iefRgkJf5xOAwB8sHyV2YUZmdc/W/PB2LGTDuw/vXrV+tra6i3frzdf2UVSG4VFBSs/XhoePvSX3YfefWfFgweFG/5njbWkWjaiSm6g4jasJivnDxqVPn/OBheJj6uz34ypn1RW38u9m2pONRh0L4x+vY/XAAzDIsImmUymyupCAMDVvw8MDIkZGBrNZvOHDp7c1y8CJ3lmGCyqUmb3RuzA7p93jBgePf2luQKBY0jIwCWL309Pv1pwL7/rpDZy72SzWKxXXl7g4uIa+UzUNxt3zJkz31raOjGiQk9l4DXTtKTstpdnMIfzz5QooZObSOj5sDS77QJvjxDzAduBDwBoVStMJlNDU7mLs2/bNZ7uQTjJM0N3oKrsv0bsQHFxUVBQSNvLwH7BAICCgryuk9oIHRCmVqtXfhJ/8NDeispygcAxPMxq1UGnbsMAXo26reqW8sr85asi25+UK/5tunt0NLlaozQaDUzmvw9PDIYDTvLMGA0A4LY3MSG0tLRoNBom89+RU2w2GwCgUim7SGqfQ7+AoPVff3/58oXEnVu379g8ZPAz8+ctCg0dZBV5lo3I5tMMOrVVCngUHk/k2ydsXPTC9ic5nK4WRGQxORQKVddOkkaLb/OKQWvg8OFafeApYbFYAAC1urXtjFKlBACIhOIukjpkEvlMVOQzUa/NfzMz83rK4X0ffxJ/5PB5KtUKUZzlWzObRzXo8GrRdXcJaJbV+PmE9/UbYv7jcp2cxV3tLIJhmJOjW0nZnbYzd++l4STPjFZtYPPtb/B5F9BotMB+/fPybredMR/7+Qd0kdQ+h+zszOs3rgEAxGLJuHGT31qyTNGiaGiot4o8y0bkC2l0Bl43phFRc4xG4/Ezm7VadV196cmzP3zzw9zq2vtdv2tQ6Jg7+X9l3zkPALh4ZU9pRS5O8swj37iOtF5QIzKZTInEOSMj/VZ2hl6vj4uddTXtUkrKPrlCfis7Y/uObweHDw3oGwgA6CKpjdy8nDWfrzhx8nBzszT/bu7hI/vFYolYLLGKVMvftUDM0KsNaoWWxbN+UyKbzV/+9u9/XUnakjCvrr7E2zNkRuwnj334GDPyNaVSevT0N78d+MS3T9iLE+J/P/gZTqMT5LVKJ+de0qv08twFP/+ScOPmtX2/nxw7dlJ9Q13ywaQftn/j4uIaMeTZN15/23xZF0ltzJzxSnOz9Idtm77d/BWDwYgePW7zt4lWuS93tRrY36caK0pMEj8yzm+vyqsbGsMNCOcRLaQjf/xa4+7P9R1gr+Ohjmwtnfqmu0Bs4Z+80y6+voM4Jn1va7/oJhhm8A3phZMiYKbTMEjiyXJgm2S1SoGL5Z+kWVa36QfL63Q5MLmtGst9ta4Sv7cX7nxStRb49MuYzpIMBj2VauEDenuGLJz3fWfvqi+W+gY70BgwroHRi+kqHh8xTXxoS2VnRuRxhe8vSbKYpNWqGQzLM/0oFCs/AXSmAQCg1WkYdAuLOtBonQa+RoOx/qFsxlu2WL4c0Z6ubCEQ0ftHchvrFTyJhWiJSqUJndwtvc+mWFeDvFo2aoZ1evERPeIxN6CoyWJVQ4uqGa/GbaiQVcu5HGNwJNpriAAeHwnNet+z7FaNTt3LH1yaa1pam1rGzHUmWghJ6VZIvmiDX1FaeS+uF2U1LUCtnL3ci2gh5KVbRsQwbMmmvvLKJnltpyt+2i/ScikDa41dTHy8S2Z60Egxe7mXSGQoTq+Q1/WSzcmklfKCS6W+gbQJ8zsORUbYmJ41pjw/RRQcybt8pLHhgcpEpfMlHHtch6RVrlHUq4wajdidPnFNH6ZDrxrcYKf0uFXPyZkxdZFbTYm6KLvlwe1aJptmNGJUBpVKp1JoVIDbKManAcMwvc5g1Or1WoO2Vcd0oASEcfsNlqCVEeHhCZuXXX1Yrj6s4bFLafUMAAABBUlEQVTiphqtrEGnlOuVMr1BbzToYTQig4VRqBQOn83mU8UeDK7A/mrxXs/T9nMIXRlCV1SvIJ4W1KNqT3AENLte9EDoyuwseENGtCccOJSGSg3RKp4QndZYUagUiC3fP5ER7QmXPiydxl4X5Wmq0XQxxBMZ0Z7w6sfGMHDrol0uVnbx96rnX+x00Xy49mtGdIfLh+t1OpP/QL7I3Q5W1VfK9bJ6zV/7a/7ziTen8/YKZES7JPdvWd41uVpl0OC2MoxVkHgwm+u0vgM4z08Rd72dJTKiHWMyAa0aaiOajCYWp1sdV8iICChADysIKEBGREABMiICCpAREVCAjIiAAmREBBT8LxNhB/DtPHnJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000210D2205E50>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import END, StateGraph, add_messages\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# LLM initialization.\n",
    "llm = ChatGroq(\n",
    "    groq_api_key = settings.groq_api_key,\n",
    "    temperature = 0,\n",
    "    max_tokens = 3000,\n",
    "    model = \"deepseek-r1-distill-llama-70b\" \n",
    ")\n",
    "\n",
    "# Bind all the custom tools to the LLM.\n",
    "llm_with_tools = llm.bind_tools(tools = available_tools)\n",
    "# Create a ToolNode to execute appropriate tools.\n",
    "tool_node: ToolNode = ToolNode(\n",
    "    tools = available_tools,\n",
    "    messages_key = \"messages\"\n",
    ")\n",
    "\n",
    "CHATBOT = \"chatbot\"\n",
    "TOOLS = \"tools\"\n",
    "\n",
    "\n",
    "class BasicChatBotState(TypedDict):\n",
    "    \"Represents the state of basic ChatBot\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def chatbot_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Execute tools and return the results.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke(state[\"messages\"])]\n",
    "    }\n",
    "\n",
    "def tool_executor_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    tool_response: List[ToolMessage] = tool_node.invoke(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": tool_response\n",
    "    }    \n",
    "\n",
    "def tool_call_required(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Check if the last message requires tool calls.\"\"\"\n",
    "    last_ai_message: AIMessage = state[\"messages\"][-1]\n",
    "    if last_ai_message.tool_calls:\n",
    "        return TOOLS\n",
    "    return END \n",
    "\n",
    "\n",
    "# Build the Graph Workflow.\n",
    "graph_builder = StateGraph(BasicChatBotState)\n",
    "graph_builder.add_node(CHATBOT, chatbot_node)\n",
    "graph_builder.add_node(TOOLS, tool_executor_node)\n",
    "graph_builder.set_entry_point(CHATBOT)\n",
    "graph_builder.add_edge(TOOLS, CHATBOT)\n",
    "graph_builder.add_conditional_edges(\n",
    "    CHATBOT,\n",
    "    tool_call_required,\n",
    "    path_map = {\n",
    "        TOOLS: TOOLS,\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(CHATBOT, END)\n",
    "\n",
    "# Memory\n",
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer = memory)\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46e4b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, this is Nagarjun. I'm an AI Engineer.\n",
      "\n",
      "AI: It seems like you're introducing yourself, but I'm not sure how I can assist you yet. Could you please provide more details about what you need help with? Whether it's a math problem, a search query, or something else, feel free to ask!\n",
      "\n",
      "Human: what's the current time ?\n",
      "\n",
      "AI: The current time is **2025-07-28 02:06:22 PM**.\n",
      "\n",
      "Human: what is the  result of 8976/0.9989878 = ?\n",
      "\n",
      "AI: The result of dividing 8976 by 0.9989878 is **8985.094712868366**.\n",
      "\n",
      "Human: what is my first question?\n",
      "\n",
      "AI: Could you clarify what you mean by \"my first question\"? Are you referring to the first message you sent, which was introducing yourself, or the first actual question you asked about the current time? Let me know so I can assist you better!\n",
      "\n",
      "Human: why LangChain and LangGraph are so popular?\n",
      "\n",
      "AI: LangChain and LangGraph are popular due to their unique strengths in the AI development ecosystem:\n",
      "\n",
      "1. **LangChain**:\n",
      "   - **Ease of Use**: LangChain is known for its simplicity and accessibility, making it ideal for quick prototyping and straightforward AI applications.\n",
      "   - **Popularity and Ecosystem**: It has a larger ecosystem and strong community support, which contributes to its widespread adoption.\n",
      "   - **Use Cases**: Suitable for developers new to AI workflows or projects with linear processes.\n",
      "\n",
      "2. **LangGraph**:\n",
      "   - **Flexibility and Autonomy**: LangGraph excels in handling complex, non-linear AI workflows, offering more control and scalability for advanced use cases.\n",
      "   - **Use Cases**: Preferred for highly complex tasks or when maximum flexibility is required.\n",
      "\n",
      "In summary, LangChain is more popular and widely adopted due to its ease of use and extensive community support, while LangGraph is chosen for its ability to handle sophisticated and complex AI workflows. The choice between the two depends on the specific needs of the project.\n",
      "\n",
      "Human: what was your last reaponse. I'm what is our last conversation topic?\n",
      "\n",
      "AI: Our last conversation topic was about **why LangChain and LangGraph are so popular**, and my last response explained their unique strengths and popularity in the AI development ecosystem. Let me know if you'd like further clarification or assistance!\n",
      "\n",
      "Human: exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input: str = input(\"Human: \")\n",
    "    print(f\"Human: {user_input}\", end = \"\\n\\n\")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"end\"]:\n",
    "        break\n",
    "    response: BasicChatBotState = graph.invoke(\n",
    "        BasicChatBotState(\n",
    "            messages = [\n",
    "                HumanMessage(content = user_input)\n",
    "            ],\n",
    "        ), \n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"my_thread\"\n",
    "            }\n",
    "        } \n",
    "    ) \n",
    "    answer: str = response[\"messages\"][-1].content\n",
    "    print(f\"AI: {answer}\", end = \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50161c4",
   "metadata": {},
   "source": [
    "### Now see, we achieve the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c3ee212",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"my_thread\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e23a047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi, this is Nagarjun. I'm an AI Engineer.\", additional_kwargs={}, response_metadata={}, id='be1849f8-615c-4f9b-852a-a5767f3fed4b'),\n",
       "  AIMessage(content=\"It seems like you're introducing yourself, but I'm not sure how I can assist you yet. Could you please provide more details about what you need help with? Whether it's a math problem, a search query, or something else, feel free to ask!\", additional_kwargs={'reasoning_content': \"Okay, so I'm trying to figure out how to use the tools provided to help with the user's query. The user mentioned they're an AI Engineer, but didn't specify what they need assistance with. Since the tools include functions for basic arithmetic operations, date/time, and a search engine, I should consider which of these might be useful.\\n\\nFirst, I need to understand the user's intent. If they're introducing themselves, maybe they don't need immediate assistance but might have a question coming up. Since they didn't ask anything specific yet, perhaps I should prompt them for more details.\\n\\nAlternatively, if they have a math problem, I can use addition, subtraction, multiplication, or division. If they need the current date and time, I can use the date_time tool. If they have a question that requires up-to-date information, the tavily_search tool would be appropriate.\\n\\nBut since the user hasn't provided a specific query yet, I should probably wait for more information before deciding which tool to use. Maybe I should ask them how I can assist them today.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 2111, 'total_tokens': 2386, 'completion_time': 1.25464387, 'prompt_time': 0.159053814, 'queue_time': 0.059716459, 'total_time': 1.413697684}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2812ffca-6780-4865-a780-3ea8b42dd6b3-0', usage_metadata={'input_tokens': 2111, 'output_tokens': 275, 'total_tokens': 2386}),\n",
       "  HumanMessage(content=\"what's the current time ?\", additional_kwargs={}, response_metadata={}, id='0cc04adc-d019-48ab-840c-f6afa7cd4ec4'),\n",
       "  AIMessage(content=\"To get the current date and time, I'll use the date_time tool.\\n\\n\", additional_kwargs={'reasoning_content': 'Alright, the user is asking for the current time. I need to figure out the best way to provide that information. Since I don\\'t have real-time access myself, I should use the tools available to me. Looking at the tools provided, there\\'s a function called \"date_time\" that can get the current date and time. \\n\\nI remember that this function doesn\\'t require any parameters, so I can call it without any arguments. By using this tool, I can retrieve the current time accurately and share it with the user. \\n\\nI should make sure to format the tool call correctly, using the function name and an empty arguments object since no parameters are needed. This will ensure that the tool executes properly and gives me the information the user is asking for.\\n', 'tool_calls': [{'id': 'rnf6pmayy', 'function': {'arguments': '{}', 'name': 'date_time'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 2173, 'total_tokens': 2365, 'completion_time': 0.847369769, 'prompt_time': 0.163735697, 'queue_time': 0.054096646, 'total_time': 1.011105466}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d62e50f7-5ec3-4dc5-b566-de5dd5622d91-0', tool_calls=[{'name': 'date_time', 'args': {}, 'id': 'rnf6pmayy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2173, 'output_tokens': 192, 'total_tokens': 2365}),\n",
       "  ToolMessage(content='2025-07-28 02:06:22, PM', name='date_time', id='115eaec1-69a6-41c3-a972-e30efd540dc1', tool_call_id='rnf6pmayy'),\n",
       "  AIMessage(content='The current time is **2025-07-28 02:06:22 PM**.', additional_kwargs={'reasoning_content': \"Alright, the user is asking for the current time. I remember that I have a tool called date_time that can provide the current date and time. I should use that tool to get the accurate time and share it with the user. Let me call the date_time function without any arguments since it doesn't require any parameters. Once I get the response, I'll format it in a user-friendly way, probably converting the 24-hour format to AM/PM for clarity. That should answer the user's query effectively.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 2234, 'total_tokens': 2363, 'completion_time': 0.554797909, 'prompt_time': 0.168143142, 'queue_time': 0.05324015900000001, 'total_time': 0.722941051}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d63b1b1d-56a2-43b5-a2ac-a07523d9986d-0', usage_metadata={'input_tokens': 2234, 'output_tokens': 129, 'total_tokens': 2363}),\n",
       "  HumanMessage(content='what is the  result of 8976/0.9989878 = ?', additional_kwargs={}, response_metadata={}, id='56051ce0-3431-462b-adf3-927c9f7e895b'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': \"Alright, the user is asking for the result of 8976 divided by 0.9989878. Hmm, that's a division problem. I remember that division by a number close to 1 might result in a value slightly higher than the dividend. Let me think about how to approach this.\\n\\nFirst, I need to use the division tool provided. The function requires two numbers: num_1 and num_2. So, num_1 is 8976 and num_2 is 0.9989878. I should make sure both are correctly formatted as numbers.\\n\\nI should call the division tool with these values. Once I get the result, I'll present it clearly to the user. Let me double-check the numbers to ensure accuracy. Yep, everything looks good. I'll go ahead and perform the division.\\n\", 'tool_calls': [{'id': 'eme2d0kk8', 'function': {'arguments': '{\"num_1\":8976,\"num_2\":0.9989878}', 'name': 'division'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 203, 'prompt_tokens': 2274, 'total_tokens': 2477, 'completion_time': 0.856377379, 'prompt_time': 0.170648064, 'queue_time': 0.059349556000000026, 'total_time': 1.027025443}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ea094802-2ab6-4591-a434-a5fb0a51da09-0', tool_calls=[{'name': 'division', 'args': {'num_1': 8976, 'num_2': 0.9989878}, 'id': 'eme2d0kk8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2274, 'output_tokens': 203, 'total_tokens': 2477}),\n",
       "  ToolMessage(content='8985.094712868366', name='division', id='da094915-d21b-4a36-b221-5b97ccd89dc2', tool_call_id='eme2d0kk8'),\n",
       "  AIMessage(content='The result of dividing 8976 by 0.9989878 is **8985.094712868366**.', additional_kwargs={'reasoning_content': \"Alright, let's tackle this problem step by step. So, the user is asking for the result of dividing 8976 by 0.9989878. Hmm, division can sometimes be tricky, especially when dealing with decimals. First, I need to understand what exactly the user is asking for. They've provided two numbers: 8976 and 0.9989878. The operation is division, so I need to divide 8976 by 0.9989878.\\n\\nOkay, so division by a decimal can be a bit confusing. I remember that dividing by a number less than 1 will actually increase the value of the dividend. So, in this case, dividing 8976 by a number slightly less than 1 should give a result slightly larger than 8976. That makes sense because if you divide something into smaller groups, each group ends up with more.\\n\\nNow, to perform the actual calculation, I could do this manually, but that might be time-consuming and prone to errors. Instead, I think using a calculator or a computational tool would be more efficient and accurate. Since I have access to a division tool, I can utilize that to get the precise result.\\n\\nBefore I proceed, I should double-check the numbers to ensure there are no typos or mistakes. The dividend is 8976, and the divisor is 0.9989878. Both numbers seem to be correctly entered. Also, I need to confirm that I'm using the correct operation—division in this case.\\n\\nOnce I'm confident that the numbers and operation are correct, I can input them into the division tool. The tool will handle the computation and provide the exact result. This approach saves time and minimizes the chance of human error.\\n\\nAfter performing the division, I should review the result to ensure it makes sense. As I mentioned earlier, since we're dividing by a number less than 1, the result should be slightly larger than 8976. If the result seems way off, I might need to double-check the calculation or verify if there was a mistake in the input.\\n\\nFinally, I can present the result to the user in a clear and understandable format. It's important to communicate the answer accurately, especially since the user might be using this information for further calculations or decision-making.\\n\\nIn summary, the steps I took were:\\n1. Understand the problem and confirm the operation.\\n2. Recognize that dividing by a decimal less than 1 increases the dividend.\\n3. Use a reliable tool to perform the division accurately.\\n4. Verify the result makes sense based on the properties of division.\\n5. Present the result clearly to the user.\\n\\nThis systematic approach ensures that the solution is both accurate and easy to understand.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 582, 'prompt_tokens': 2342, 'total_tokens': 2924, 'completion_time': 2.293857808, 'prompt_time': 0.174247183, 'queue_time': 0.06292477699999999, 'total_time': 2.468104991}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5df80ccc-ee10-461e-8504-99edefa49baa-0', usage_metadata={'input_tokens': 2342, 'output_tokens': 582, 'total_tokens': 2924}),\n",
       "  HumanMessage(content='what is my first question?', additional_kwargs={}, response_metadata={}, id='93baa4c5-a2e7-422d-b85e-3f0e8cb6403a'),\n",
       "  AIMessage(content='Could you clarify what you mean by \"my first question\"? Are you referring to the first message you sent, which was introducing yourself, or the first actual question you asked about the current time? Let me know so I can assist you better!', additional_kwargs={'reasoning_content': 'Alright, let me try to figure out what the user is asking for here. So, the user just asked, \"what is my first question?\" Looking back at the conversation history, the first thing they said was introducing themselves as Nagarjun, an AI Engineer. Then they asked about the current time, followed by a division problem. \\n\\nHmm, so when they\\'re asking for their first question, they might be referring to the very first message they sent, which was their introduction. But that wasn\\'t really a question; it was more of a statement. Alternatively, they might be asking about the first actual question they posed, which would be the one about the current time.\\n\\nI should clarify this to make sure I understand correctly. Maybe they\\'re trying to track their interactions or perhaps they\\'re testing if I can recall past messages. Either way, it\\'s best to ask for a bit more context to provide the most accurate response.\\n'}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2376, 'total_tokens': 2617, 'completion_time': 1.148628198, 'prompt_time': 0.176982329, 'queue_time': 0.054975090000000004, 'total_time': 1.325610527}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--24a88e74-2c86-48f1-b436-334b28d75968-0', usage_metadata={'input_tokens': 2376, 'output_tokens': 241, 'total_tokens': 2617}),\n",
       "  HumanMessage(content='why LangChain and LangGraph are so popular?', additional_kwargs={}, response_metadata={}, id='4770cde0-f13f-4389-8525-0793e0a69f6a'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Alright, the user is asking why LangChain and LangGraph are so popular. I need to provide a detailed explanation. First, I should consider the key features and benefits of each tool.\\n\\nLangChain is known for its flexibility and modularity. It allows developers to build applications that integrate various AI models and tools seamlessly. Its community-driven aspect means there are many resources and pre-built components available, which lowers the barrier to entry for developers.\\n\\nOn the other hand, LangGraph focuses on visualizing and managing complex AI workflows. Its graph-based interface makes it easier to design and debug sophisticated applications, which is especially useful for enterprises that need scalable and maintainable solutions.\\n\\nThe rise of generative AI has created a surge in demand for tools that can effectively harness and manage AI capabilities. Both LangChain and LangGraph tap into this demand by providing frameworks that simplify the development process, allowing developers to focus on innovation rather than infrastructure.\\n\\nAdditionally, the strong community support and continuous updates for both tools contribute to their popularity. They are seen as essential for building the next generation of AI-driven applications, making them indispensable in the current tech landscape.\\n', 'tool_calls': [{'id': 'tavily_search', 'function': {'arguments': '{\"end_date\":null,\"exclude_domains\":[],\"include_domains\":[],\"include_favicon\":false,\"include_images\":false,\"query\":\"why LangChain and LangGraph are so popular\",\"search_depth\":\"advanced\",\"start_date\":null,\"time_range\":null,\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 306, 'prompt_tokens': 2438, 'total_tokens': 2744, 'completion_time': 1.38033598, 'prompt_time': 0.184988119, 'queue_time': 0.054695978000000006, 'total_time': 1.5653240990000001}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5150a76e-7862-48fa-80bf-9495a7880d0d-0', tool_calls=[{'name': 'tavily_search', 'args': {'end_date': None, 'exclude_domains': [], 'include_domains': [], 'include_favicon': False, 'include_images': False, 'query': 'why LangChain and LangGraph are so popular', 'search_depth': 'advanced', 'start_date': None, 'time_range': None, 'topic': 'general'}, 'id': 'tavily_search', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2438, 'output_tokens': 306, 'total_tokens': 2744}),\n",
       "  ToolMessage(content='{\"query\": \"why LangChain and LangGraph are so popular\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://aiagentstore.ai/compare-ai-agents/langchain-vs-langgraph\", \"title\": \"Agentic AI Comparison: LangChain vs LangGraph\", \"content\": \"LangChain and LangGraph serve different needs in the AI development ecosystem. LangChain excels in ease of use and popularity, making it an excellent choice for quick prototyping and straightforward AI applications. It\\'s particularly suitable for developers new to AI workflows or those working on projects with linear processes. On the other hand, LangGraph shines in autonomy and flexibility, making it the go-to framework for complex, non-linear AI workflows. While it has a steeper learning [...] LangChain is significantly more popular and widely adopted, benefiting from a larger ecosystem and community support. LangGraph is growing but currently less widespread.\\\\n\\\\n## Conclusions [...] curve, LangGraph offers unparalleled control and scalability for advanced use cases. The choice between the two depends on the specific requirements of the project, the complexity of the workflow, and the developer\\'s expertise. For simple to moderately complex projects, LangChain is often the more practical choice due to its ease of use and extensive community support. For highly complex, state-dependent workflows or when maximum flexibility is required, LangGraph is the superior option.\", \"score\": 0.8785752, \"raw_content\": null}, {\"url\": \"https://thirdeyedata.ai/a-comparative-study-between-langgraph-and-langchain-for-enterprise-ai-development/\", \"title\": \"A Comparative Study Between LangGraph and LangChain for ...\", \"content\": \"The choice between LangGraph and LangChain represents more than a technical decision – it’s a strategic choice that will influence your organization’s AI development trajectory. LangChain remains an excellent choice for rapid development and straightforward applications, while LangGraph opens the door to sophisticated, production-grade AI systems that can handle complex real-world requirements.\", \"score\": 0.7950386, \"raw_content\": null}], \"response_time\": 2.06}', name='tavily_search', id='9861d4b5-cafc-49f9-9a43-375779737b05', tool_call_id='tavily_search'),\n",
       "  AIMessage(content='LangChain and LangGraph are popular due to their unique strengths in the AI development ecosystem:\\n\\n1. **LangChain**:\\n   - **Ease of Use**: LangChain is known for its simplicity and accessibility, making it ideal for quick prototyping and straightforward AI applications.\\n   - **Popularity and Ecosystem**: It has a larger ecosystem and strong community support, which contributes to its widespread adoption.\\n   - **Use Cases**: Suitable for developers new to AI workflows or projects with linear processes.\\n\\n2. **LangGraph**:\\n   - **Flexibility and Autonomy**: LangGraph excels in handling complex, non-linear AI workflows, offering more control and scalability for advanced use cases.\\n   - **Use Cases**: Preferred for highly complex tasks or when maximum flexibility is required.\\n\\nIn summary, LangChain is more popular and widely adopted due to its ease of use and extensive community support, while LangGraph is chosen for its ability to handle sophisticated and complex AI workflows. The choice between the two depends on the specific needs of the project.', additional_kwargs={'reasoning_content': \"Alright, so the user is asking why LangChain and LangGraph are so popular. I remember from the previous conversation that they were inquiring about the current time and some math problems, but now they're shifting focus to AI tools. \\n\\nFirst, I need to understand what LangChain and LangGraph are. From what I know, LangChain is a framework for building AI applications, especially those that involve language models. It's known for being user-friendly and great for prototyping. On the other hand, LangGraph seems to be another tool in the same space but perhaps more focused on complex workflows.\\n\\nThe user's query suggests they're interested in the popularity aspects. So, I should consider factors like ease of use, community support, and use cases. LangChain's popularity might stem from its simplicity and large community, making it accessible for developers. LangGraph, while powerful, might be less popular because it's more complex and possibly newer.\\n\\nI should also think about the user's potential background. They might be a developer or someone interested in AI, looking to choose between these tools for a project. They might not just want to know why they're popular but also which one to use.\\n\\nLooking at the search results provided, it seems LangChain is more popular due to its ease of use and larger ecosystem, while LangGraph is preferred for complex tasks. This aligns with what I thought earlier.\\n\\nSo, in my response, I should highlight the key points: LangChain's ease of use, community support, and suitability for simpler projects, versus LangGraph's flexibility and scalability for more complex needs. This way, the user gets a clear understanding of why each tool is popular and which might be better suited for their specific use case.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 563, 'prompt_tokens': 2980, 'total_tokens': 3543, 'completion_time': 2.312520475, 'prompt_time': 0.223515406, 'queue_time': 0.05453681200000002, 'total_time': 2.536035881}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--181e03db-ec4f-4894-91a1-3010bc7d9382-0', usage_metadata={'input_tokens': 2980, 'output_tokens': 563, 'total_tokens': 3543}),\n",
       "  HumanMessage(content=\"what was your last reaponse. I'm what is our last conversation topic?\", additional_kwargs={}, response_metadata={}, id='a49852fa-cb9a-4cd4-92b1-8a46690fe410'),\n",
       "  AIMessage(content=\"Our last conversation topic was about **why LangChain and LangGraph are so popular**, and my last response explained their unique strengths and popularity in the AI development ecosystem. Let me know if you'd like further clarification or assistance!\", additional_kwargs={'reasoning_content': 'Alright, let me try to figure out what the user is asking for here. They just said, \"what was your last reaponse. I\\'m what is our last conversation topic?\" Hmm, I think there might be a typo there. Maybe they meant \"response\" instead of \"reaponse.\" So, they\\'re asking about my last response and the topic of our last conversation.\\n\\nLooking back at our conversation history, the last thing I did was explain why LangChain and LangGraph are popular. I provided a detailed answer based on the search results. So, the last response was about that topic, and the conversation was focused on comparing LangChain and LangGraph.\\n\\nI should make sure to clarify that in my response. Maybe I\\'ll mention that the last response was about LangChain and LangGraph\\'s popularity and that the conversation topic was comparing these two AI frameworks. I should also offer further assistance in case they have more questions or need additional information.\\n\\nI need to keep it clear and concise, avoiding any markdown as per the instructions. Also, I should maintain a friendly and helpful tone to ensure the user feels supported.\\n'}, response_metadata={'token_usage': {'completion_tokens': 277, 'prompt_tokens': 3208, 'total_tokens': 3485, 'completion_time': 1.338079746, 'prompt_time': 0.240111971, 'queue_time': 0.05520766200000002, 'total_time': 1.5781917170000002}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2acaffc8-c3ca-4296-84dc-71af6c1d0dd9-0', usage_metadata={'input_tokens': 3208, 'output_tokens': 277, 'total_tokens': 3485})]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2572d36",
   "metadata": {},
   "source": [
    "How checkpointing works in LangGraph and how the LLM uses it to remember conversations.\n",
    "\n",
    "## How Checkpointing Works in LangGraph\n",
    "\n",
    "**1. Where is it stored?**\n",
    "`InMemorySaver()` stores checkpoints in **RAM memory** of your Python process. It's not persistent - when your program ends, the memory is lost. The checkpoints are stored as serialized state snapshots, not in your `state[\"messages\"]` directly.\n",
    "\n",
    "**2. What gets stored?**\n",
    "Every time a node completes execution, LangGraph automatically saves the **entire state** as a checkpoint. In your case, this means the complete `messages` list gets serialized and stored with a unique checkpoint ID.\n",
    "\n",
    "**3. How does the LLM use this memory?**\n",
    "Here's the crucial part: **The LLM itself doesn't directly access the checkpointer**. Instead, LangGraph uses the `thread_id` to:\n",
    "- Retrieve the last checkpoint for that thread\n",
    "- Load the previous `state[\"messages\"]` \n",
    "- **Append your new message** to the existing messages list (thanks to the `add_messages` reducer)\n",
    "- Pass the **complete conversation history** to the LLM\n",
    "\n",
    "## The Memory Flow\n",
    "\n",
    "When you invoke the graph with a `thread_id`, here's what happens:\n",
    "\n",
    "1. **Retrieval**: LangGraph looks up the last checkpoint for \"my_thread\"\n",
    "2. **State Reconstruction**: Loads the previous `messages: [msg1, msg2, msg3, ...]`\n",
    "3. **Message Addition**: Your new `HumanMessage` gets added: `[msg1, msg2, msg3, ..., new_msg]`\n",
    "4. **LLM Context**: The LLM receives the **entire message history**, not just your current input\n",
    "5. **New Checkpoint**: After processing, the updated state gets saved as a new checkpoint\n",
    "\n",
    "\n",
    "This creates a fresh state with only the current message. To properly use memory, you should invoke with just the new message and let LangGraph handle the state reconstruction:\n",
    "\n",
    "```python\n",
    "response = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content = user_input)]},  # Just the new message\n",
    "    config = {\"configurable\": {\"thread_id\": \"my_thread\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "## Memory Persistence Options\n",
    "\n",
    "- **InMemorySaver**: RAM only, lost when program ends\n",
    "- **SqliteSaver**: Persists to SQLite database file\n",
    "- **PostgresSaver**: Persists to PostgreSQL database\n",
    "\n",
    "The checkpointer acts as a \"time machine\" for your conversation state, allowing the LLM to have context about the entire conversation history, not just the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "82e32fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hi, this is Nagarjun. I'm an AI Engineer.\" additional_kwargs={} response_metadata={} id='be1849f8-615c-4f9b-852a-a5767f3fed4b'\n",
      "content=\"It seems like you're introducing yourself, but I'm not sure how I can assist you yet. Could you please provide more details about what you need help with? Whether it's a math problem, a search query, or something else, feel free to ask!\" additional_kwargs={'reasoning_content': \"Okay, so I'm trying to figure out how to use the tools provided to help with the user's query. The user mentioned they're an AI Engineer, but didn't specify what they need assistance with. Since the tools include functions for basic arithmetic operations, date/time, and a search engine, I should consider which of these might be useful.\\n\\nFirst, I need to understand the user's intent. If they're introducing themselves, maybe they don't need immediate assistance but might have a question coming up. Since they didn't ask anything specific yet, perhaps I should prompt them for more details.\\n\\nAlternatively, if they have a math problem, I can use addition, subtraction, multiplication, or division. If they need the current date and time, I can use the date_time tool. If they have a question that requires up-to-date information, the tavily_search tool would be appropriate.\\n\\nBut since the user hasn't provided a specific query yet, I should probably wait for more information before deciding which tool to use. Maybe I should ask them how I can assist them today.\\n\"} response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 2111, 'total_tokens': 2386, 'completion_time': 1.25464387, 'prompt_time': 0.159053814, 'queue_time': 0.059716459, 'total_time': 1.413697684}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--2812ffca-6780-4865-a780-3ea8b42dd6b3-0' usage_metadata={'input_tokens': 2111, 'output_tokens': 275, 'total_tokens': 2386}\n",
      "content=\"what's the current time ?\" additional_kwargs={} response_metadata={} id='0cc04adc-d019-48ab-840c-f6afa7cd4ec4'\n",
      "content=\"To get the current date and time, I'll use the date_time tool.\\n\\n\" additional_kwargs={'reasoning_content': 'Alright, the user is asking for the current time. I need to figure out the best way to provide that information. Since I don\\'t have real-time access myself, I should use the tools available to me. Looking at the tools provided, there\\'s a function called \"date_time\" that can get the current date and time. \\n\\nI remember that this function doesn\\'t require any parameters, so I can call it without any arguments. By using this tool, I can retrieve the current time accurately and share it with the user. \\n\\nI should make sure to format the tool call correctly, using the function name and an empty arguments object since no parameters are needed. This will ensure that the tool executes properly and gives me the information the user is asking for.\\n', 'tool_calls': [{'id': 'rnf6pmayy', 'function': {'arguments': '{}', 'name': 'date_time'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 2173, 'total_tokens': 2365, 'completion_time': 0.847369769, 'prompt_time': 0.163735697, 'queue_time': 0.054096646, 'total_time': 1.011105466}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--d62e50f7-5ec3-4dc5-b566-de5dd5622d91-0' tool_calls=[{'name': 'date_time', 'args': {}, 'id': 'rnf6pmayy', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2173, 'output_tokens': 192, 'total_tokens': 2365}\n",
      "content='2025-07-28 02:06:22, PM' name='date_time' id='115eaec1-69a6-41c3-a972-e30efd540dc1' tool_call_id='rnf6pmayy'\n",
      "content='The current time is **2025-07-28 02:06:22 PM**.' additional_kwargs={'reasoning_content': \"Alright, the user is asking for the current time. I remember that I have a tool called date_time that can provide the current date and time. I should use that tool to get the accurate time and share it with the user. Let me call the date_time function without any arguments since it doesn't require any parameters. Once I get the response, I'll format it in a user-friendly way, probably converting the 24-hour format to AM/PM for clarity. That should answer the user's query effectively.\\n\"} response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 2234, 'total_tokens': 2363, 'completion_time': 0.554797909, 'prompt_time': 0.168143142, 'queue_time': 0.05324015900000001, 'total_time': 0.722941051}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--d63b1b1d-56a2-43b5-a2ac-a07523d9986d-0' usage_metadata={'input_tokens': 2234, 'output_tokens': 129, 'total_tokens': 2363}\n",
      "content='what is the  result of 8976/0.9989878 = ?' additional_kwargs={} response_metadata={} id='56051ce0-3431-462b-adf3-927c9f7e895b'\n",
      "content='' additional_kwargs={'reasoning_content': \"Alright, the user is asking for the result of 8976 divided by 0.9989878. Hmm, that's a division problem. I remember that division by a number close to 1 might result in a value slightly higher than the dividend. Let me think about how to approach this.\\n\\nFirst, I need to use the division tool provided. The function requires two numbers: num_1 and num_2. So, num_1 is 8976 and num_2 is 0.9989878. I should make sure both are correctly formatted as numbers.\\n\\nI should call the division tool with these values. Once I get the result, I'll present it clearly to the user. Let me double-check the numbers to ensure accuracy. Yep, everything looks good. I'll go ahead and perform the division.\\n\", 'tool_calls': [{'id': 'eme2d0kk8', 'function': {'arguments': '{\"num_1\":8976,\"num_2\":0.9989878}', 'name': 'division'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 203, 'prompt_tokens': 2274, 'total_tokens': 2477, 'completion_time': 0.856377379, 'prompt_time': 0.170648064, 'queue_time': 0.059349556000000026, 'total_time': 1.027025443}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--ea094802-2ab6-4591-a434-a5fb0a51da09-0' tool_calls=[{'name': 'division', 'args': {'num_1': 8976, 'num_2': 0.9989878}, 'id': 'eme2d0kk8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2274, 'output_tokens': 203, 'total_tokens': 2477}\n",
      "content='8985.094712868366' name='division' id='da094915-d21b-4a36-b221-5b97ccd89dc2' tool_call_id='eme2d0kk8'\n",
      "content='The result of dividing 8976 by 0.9989878 is **8985.094712868366**.' additional_kwargs={'reasoning_content': \"Alright, let's tackle this problem step by step. So, the user is asking for the result of dividing 8976 by 0.9989878. Hmm, division can sometimes be tricky, especially when dealing with decimals. First, I need to understand what exactly the user is asking for. They've provided two numbers: 8976 and 0.9989878. The operation is division, so I need to divide 8976 by 0.9989878.\\n\\nOkay, so division by a decimal can be a bit confusing. I remember that dividing by a number less than 1 will actually increase the value of the dividend. So, in this case, dividing 8976 by a number slightly less than 1 should give a result slightly larger than 8976. That makes sense because if you divide something into smaller groups, each group ends up with more.\\n\\nNow, to perform the actual calculation, I could do this manually, but that might be time-consuming and prone to errors. Instead, I think using a calculator or a computational tool would be more efficient and accurate. Since I have access to a division tool, I can utilize that to get the precise result.\\n\\nBefore I proceed, I should double-check the numbers to ensure there are no typos or mistakes. The dividend is 8976, and the divisor is 0.9989878. Both numbers seem to be correctly entered. Also, I need to confirm that I'm using the correct operation—division in this case.\\n\\nOnce I'm confident that the numbers and operation are correct, I can input them into the division tool. The tool will handle the computation and provide the exact result. This approach saves time and minimizes the chance of human error.\\n\\nAfter performing the division, I should review the result to ensure it makes sense. As I mentioned earlier, since we're dividing by a number less than 1, the result should be slightly larger than 8976. If the result seems way off, I might need to double-check the calculation or verify if there was a mistake in the input.\\n\\nFinally, I can present the result to the user in a clear and understandable format. It's important to communicate the answer accurately, especially since the user might be using this information for further calculations or decision-making.\\n\\nIn summary, the steps I took were:\\n1. Understand the problem and confirm the operation.\\n2. Recognize that dividing by a decimal less than 1 increases the dividend.\\n3. Use a reliable tool to perform the division accurately.\\n4. Verify the result makes sense based on the properties of division.\\n5. Present the result clearly to the user.\\n\\nThis systematic approach ensures that the solution is both accurate and easy to understand.\\n\"} response_metadata={'token_usage': {'completion_tokens': 582, 'prompt_tokens': 2342, 'total_tokens': 2924, 'completion_time': 2.293857808, 'prompt_time': 0.174247183, 'queue_time': 0.06292477699999999, 'total_time': 2.468104991}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5df80ccc-ee10-461e-8504-99edefa49baa-0' usage_metadata={'input_tokens': 2342, 'output_tokens': 582, 'total_tokens': 2924}\n",
      "content='what is my first question?' additional_kwargs={} response_metadata={} id='93baa4c5-a2e7-422d-b85e-3f0e8cb6403a'\n",
      "content='Could you clarify what you mean by \"my first question\"? Are you referring to the first message you sent, which was introducing yourself, or the first actual question you asked about the current time? Let me know so I can assist you better!' additional_kwargs={'reasoning_content': 'Alright, let me try to figure out what the user is asking for here. So, the user just asked, \"what is my first question?\" Looking back at the conversation history, the first thing they said was introducing themselves as Nagarjun, an AI Engineer. Then they asked about the current time, followed by a division problem. \\n\\nHmm, so when they\\'re asking for their first question, they might be referring to the very first message they sent, which was their introduction. But that wasn\\'t really a question; it was more of a statement. Alternatively, they might be asking about the first actual question they posed, which would be the one about the current time.\\n\\nI should clarify this to make sure I understand correctly. Maybe they\\'re trying to track their interactions or perhaps they\\'re testing if I can recall past messages. Either way, it\\'s best to ask for a bit more context to provide the most accurate response.\\n'} response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2376, 'total_tokens': 2617, 'completion_time': 1.148628198, 'prompt_time': 0.176982329, 'queue_time': 0.054975090000000004, 'total_time': 1.325610527}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--24a88e74-2c86-48f1-b436-334b28d75968-0' usage_metadata={'input_tokens': 2376, 'output_tokens': 241, 'total_tokens': 2617}\n",
      "content='why LangChain and LangGraph are so popular?' additional_kwargs={} response_metadata={} id='4770cde0-f13f-4389-8525-0793e0a69f6a'\n",
      "content='' additional_kwargs={'reasoning_content': 'Alright, the user is asking why LangChain and LangGraph are so popular. I need to provide a detailed explanation. First, I should consider the key features and benefits of each tool.\\n\\nLangChain is known for its flexibility and modularity. It allows developers to build applications that integrate various AI models and tools seamlessly. Its community-driven aspect means there are many resources and pre-built components available, which lowers the barrier to entry for developers.\\n\\nOn the other hand, LangGraph focuses on visualizing and managing complex AI workflows. Its graph-based interface makes it easier to design and debug sophisticated applications, which is especially useful for enterprises that need scalable and maintainable solutions.\\n\\nThe rise of generative AI has created a surge in demand for tools that can effectively harness and manage AI capabilities. Both LangChain and LangGraph tap into this demand by providing frameworks that simplify the development process, allowing developers to focus on innovation rather than infrastructure.\\n\\nAdditionally, the strong community support and continuous updates for both tools contribute to their popularity. They are seen as essential for building the next generation of AI-driven applications, making them indispensable in the current tech landscape.\\n', 'tool_calls': [{'id': 'tavily_search', 'function': {'arguments': '{\"end_date\":null,\"exclude_domains\":[],\"include_domains\":[],\"include_favicon\":false,\"include_images\":false,\"query\":\"why LangChain and LangGraph are so popular\",\"search_depth\":\"advanced\",\"start_date\":null,\"time_range\":null,\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 306, 'prompt_tokens': 2438, 'total_tokens': 2744, 'completion_time': 1.38033598, 'prompt_time': 0.184988119, 'queue_time': 0.054695978000000006, 'total_time': 1.5653240990000001}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--5150a76e-7862-48fa-80bf-9495a7880d0d-0' tool_calls=[{'name': 'tavily_search', 'args': {'end_date': None, 'exclude_domains': [], 'include_domains': [], 'include_favicon': False, 'include_images': False, 'query': 'why LangChain and LangGraph are so popular', 'search_depth': 'advanced', 'start_date': None, 'time_range': None, 'topic': 'general'}, 'id': 'tavily_search', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2438, 'output_tokens': 306, 'total_tokens': 2744}\n",
      "content='{\"query\": \"why LangChain and LangGraph are so popular\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://aiagentstore.ai/compare-ai-agents/langchain-vs-langgraph\", \"title\": \"Agentic AI Comparison: LangChain vs LangGraph\", \"content\": \"LangChain and LangGraph serve different needs in the AI development ecosystem. LangChain excels in ease of use and popularity, making it an excellent choice for quick prototyping and straightforward AI applications. It\\'s particularly suitable for developers new to AI workflows or those working on projects with linear processes. On the other hand, LangGraph shines in autonomy and flexibility, making it the go-to framework for complex, non-linear AI workflows. While it has a steeper learning [...] LangChain is significantly more popular and widely adopted, benefiting from a larger ecosystem and community support. LangGraph is growing but currently less widespread.\\\\n\\\\n## Conclusions [...] curve, LangGraph offers unparalleled control and scalability for advanced use cases. The choice between the two depends on the specific requirements of the project, the complexity of the workflow, and the developer\\'s expertise. For simple to moderately complex projects, LangChain is often the more practical choice due to its ease of use and extensive community support. For highly complex, state-dependent workflows or when maximum flexibility is required, LangGraph is the superior option.\", \"score\": 0.8785752, \"raw_content\": null}, {\"url\": \"https://thirdeyedata.ai/a-comparative-study-between-langgraph-and-langchain-for-enterprise-ai-development/\", \"title\": \"A Comparative Study Between LangGraph and LangChain for ...\", \"content\": \"The choice between LangGraph and LangChain represents more than a technical decision – it’s a strategic choice that will influence your organization’s AI development trajectory. LangChain remains an excellent choice for rapid development and straightforward applications, while LangGraph opens the door to sophisticated, production-grade AI systems that can handle complex real-world requirements.\", \"score\": 0.7950386, \"raw_content\": null}], \"response_time\": 2.06}' name='tavily_search' id='9861d4b5-cafc-49f9-9a43-375779737b05' tool_call_id='tavily_search'\n",
      "content='LangChain and LangGraph are popular due to their unique strengths in the AI development ecosystem:\\n\\n1. **LangChain**:\\n   - **Ease of Use**: LangChain is known for its simplicity and accessibility, making it ideal for quick prototyping and straightforward AI applications.\\n   - **Popularity and Ecosystem**: It has a larger ecosystem and strong community support, which contributes to its widespread adoption.\\n   - **Use Cases**: Suitable for developers new to AI workflows or projects with linear processes.\\n\\n2. **LangGraph**:\\n   - **Flexibility and Autonomy**: LangGraph excels in handling complex, non-linear AI workflows, offering more control and scalability for advanced use cases.\\n   - **Use Cases**: Preferred for highly complex tasks or when maximum flexibility is required.\\n\\nIn summary, LangChain is more popular and widely adopted due to its ease of use and extensive community support, while LangGraph is chosen for its ability to handle sophisticated and complex AI workflows. The choice between the two depends on the specific needs of the project.' additional_kwargs={'reasoning_content': \"Alright, so the user is asking why LangChain and LangGraph are so popular. I remember from the previous conversation that they were inquiring about the current time and some math problems, but now they're shifting focus to AI tools. \\n\\nFirst, I need to understand what LangChain and LangGraph are. From what I know, LangChain is a framework for building AI applications, especially those that involve language models. It's known for being user-friendly and great for prototyping. On the other hand, LangGraph seems to be another tool in the same space but perhaps more focused on complex workflows.\\n\\nThe user's query suggests they're interested in the popularity aspects. So, I should consider factors like ease of use, community support, and use cases. LangChain's popularity might stem from its simplicity and large community, making it accessible for developers. LangGraph, while powerful, might be less popular because it's more complex and possibly newer.\\n\\nI should also think about the user's potential background. They might be a developer or someone interested in AI, looking to choose between these tools for a project. They might not just want to know why they're popular but also which one to use.\\n\\nLooking at the search results provided, it seems LangChain is more popular due to its ease of use and larger ecosystem, while LangGraph is preferred for complex tasks. This aligns with what I thought earlier.\\n\\nSo, in my response, I should highlight the key points: LangChain's ease of use, community support, and suitability for simpler projects, versus LangGraph's flexibility and scalability for more complex needs. This way, the user gets a clear understanding of why each tool is popular and which might be better suited for their specific use case.\\n\"} response_metadata={'token_usage': {'completion_tokens': 563, 'prompt_tokens': 2980, 'total_tokens': 3543, 'completion_time': 2.312520475, 'prompt_time': 0.223515406, 'queue_time': 0.05453681200000002, 'total_time': 2.536035881}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--181e03db-ec4f-4894-91a1-3010bc7d9382-0' usage_metadata={'input_tokens': 2980, 'output_tokens': 563, 'total_tokens': 3543}\n",
      "content=\"what was your last reaponse. I'm what is our last conversation topic?\" additional_kwargs={} response_metadata={} id='a49852fa-cb9a-4cd4-92b1-8a46690fe410'\n",
      "content=\"Our last conversation topic was about **why LangChain and LangGraph are so popular**, and my last response explained their unique strengths and popularity in the AI development ecosystem. Let me know if you'd like further clarification or assistance!\" additional_kwargs={'reasoning_content': 'Alright, let me try to figure out what the user is asking for here. They just said, \"what was your last reaponse. I\\'m what is our last conversation topic?\" Hmm, I think there might be a typo there. Maybe they meant \"response\" instead of \"reaponse.\" So, they\\'re asking about my last response and the topic of our last conversation.\\n\\nLooking back at our conversation history, the last thing I did was explain why LangChain and LangGraph are popular. I provided a detailed answer based on the search results. So, the last response was about that topic, and the conversation was focused on comparing LangChain and LangGraph.\\n\\nI should make sure to clarify that in my response. Maybe I\\'ll mention that the last response was about LangChain and LangGraph\\'s popularity and that the conversation topic was comparing these two AI frameworks. I should also offer further assistance in case they have more questions or need additional information.\\n\\nI need to keep it clear and concise, avoiding any markdown as per the instructions. Also, I should maintain a friendly and helpful tone to ensure the user feels supported.\\n'} response_metadata={'token_usage': {'completion_tokens': 277, 'prompt_tokens': 3208, 'total_tokens': 3485, 'completion_time': 1.338079746, 'prompt_time': 0.240111971, 'queue_time': 0.05520766200000002, 'total_time': 1.5781917170000002}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--2acaffc8-c3ca-4296-84dc-71af6c1d0dd9-0' usage_metadata={'input_tokens': 3208, 'output_tokens': 277, 'total_tokens': 3485}\n"
     ]
    }
   ],
   "source": [
    "for msg in snapshot.values[\"messages\"]:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf34b0",
   "metadata": {},
   "source": [
    "## Create a new thread to interact with a Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "56f80e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the temperature in chennai now?', additional_kwargs={}, response_metadata={}, id='bf134153-8ff1-44eb-b83b-3c8005751959'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'dfdw5kk4m', 'function': {'arguments': '{\"query\":\"current temperature in chennai now\",\"topic\":\"news\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2351, 'total_tokens': 2378, 'completion_time': 0.069227334, 'prompt_time': 0.242482064, 'queue_time': 0.05240523600000002, 'total_time': 0.311709398}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--31acbf1e-b461-486d-8a1c-c6bc947f6cc2-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current temperature in chennai now', 'topic': 'news'}, 'id': 'dfdw5kk4m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2351, 'output_tokens': 27, 'total_tokens': 2378}),\n",
       "  ToolMessage(content='{\"query\": \"current temperature in chennai now\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.accuweather.com/en/in/chennai/206671/current-weather/206671\", \"title\": \"Chennai, Tamil Nadu, India Current Weather - AccuWeather\", \"content\": \"Chennai, Tamil Nadu · Current Weather. 6:35 AM. 83°F. Cloudy. RealFeel® 88°. Very Warm. RealFeel Guide. Very Warm. 82° to 89°. Older\", \"score\": 0.8396422, \"raw_content\": null}, {\"url\": \"https://weather.com/en-IN/weather/today/l/Chennai+Tamil+Nadu?canonicalCityId=e8f12d57533a42fc2b834f36c6195432\", \"title\": \"Weather forecast and conditions for Chennai, Tamil Nadu\", \"content\": \"Weather Today in Chennai, Tamil Nadu. Feels Like32°. 05:52. 18:37. High/Low. 35°/28°. Wind. 18 km/h. Humidity. 68%. Dew Point. 22°. Pressure. 1004.4 mb.\", \"score\": 0.72207695, \"raw_content\": null}], \"response_time\": 1.2}', name='tavily_search', id='1b0c2bb7-7a79-47ba-a363-fd09f0f728ba', tool_call_id='dfdw5kk4m'),\n",
       "  AIMessage(content='The current temperature in Chennai is around 83°F (28°C) according to AccuWeather.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 2683, 'total_tokens': 2704, 'completion_time': 0.048071421, 'prompt_time': 0.211209251, 'queue_time': 0.05282709800000002, 'total_time': 0.259280672}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0564c4c5-fdc1-41bf-a213-17043f4e29ad-0', usage_metadata={'input_tokens': 2683, 'output_tokens': 21, 'total_tokens': 2704})]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"What is the temperature in chennai now?\")\n",
    "        ],\n",
    "    ),\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"thread_2\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0d92082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Chennai is around 83°F (28°C) according to AccuWeather.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34c16e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'messages': [HumanMessage(content='What is the temperature in chennai now?', additional_kwargs={}, response_metadata={}, id='bf134153-8ff1-44eb-b83b-3c8005751959'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'dfdw5kk4m', 'function': {'arguments': '{\"query\":\"current temperature in chennai now\",\"topic\":\"news\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2351, 'total_tokens': 2378, 'completion_time': 0.069227334, 'prompt_time': 0.242482064, 'queue_time': 0.05240523600000002, 'total_time': 0.311709398}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--31acbf1e-b461-486d-8a1c-c6bc947f6cc2-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current temperature in chennai now', 'topic': 'news'}, 'id': 'dfdw5kk4m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2351, 'output_tokens': 27, 'total_tokens': 2378}), ToolMessage(content='{\"query\": \"current temperature in chennai now\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.accuweather.com/en/in/chennai/206671/current-weather/206671\", \"title\": \"Chennai, Tamil Nadu, India Current Weather - AccuWeather\", \"content\": \"Chennai, Tamil Nadu · Current Weather. 6:35 AM. 83°F. Cloudy. RealFeel® 88°. Very Warm. RealFeel Guide. Very Warm. 82° to 89°. Older\", \"score\": 0.8396422, \"raw_content\": null}, {\"url\": \"https://weather.com/en-IN/weather/today/l/Chennai+Tamil+Nadu?canonicalCityId=e8f12d57533a42fc2b834f36c6195432\", \"title\": \"Weather forecast and conditions for Chennai, Tamil Nadu\", \"content\": \"Weather Today in Chennai, Tamil Nadu. Feels Like32°. 05:52. 18:37. High/Low. 35°/28°. Wind. 18 km/h. Humidity. 68%. Dew Point. 22°. Pressure. 1004.4 mb.\", \"score\": 0.72207695, \"raw_content\": null}], \"response_time\": 1.2}', name='tavily_search', id='1b0c2bb7-7a79-47ba-a363-fd09f0f728ba', tool_call_id='dfdw5kk4m'), AIMessage(content='The current temperature in Chennai is around 83°F (28°C) according to AccuWeather.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 2683, 'total_tokens': 2704, 'completion_time': 0.048071421, 'prompt_time': 0.211209251, 'queue_time': 0.05282709800000002, 'total_time': 0.259280672}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0564c4c5-fdc1-41bf-a213-17043f4e29ad-0', usage_metadata={'input_tokens': 2683, 'output_tokens': 21, 'total_tokens': 2704})]}, next=(), config={'configurable': {'thread_id': 'thread_2', 'checkpoint_ns': '', 'checkpoint_id': '1f06b834-8728-661b-8003-ab1a0ba4675d'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='The current temperature in Chennai is around 83°F (28°C) according to AccuWeather.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 2683, 'total_tokens': 2704, 'completion_time': 0.048071421, 'prompt_time': 0.211209251, 'queue_time': 0.05282709800000002, 'total_time': 0.259280672}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0564c4c5-fdc1-41bf-a213-17043f4e29ad-0', usage_metadata={'input_tokens': 2683, 'output_tokens': 21, 'total_tokens': 2704})]}}, 'step': 3, 'parents': {}, 'thread_id': 'thread_2'}, created_at='2025-07-28T07:20:04.883764+00:00', parent_config={'configurable': {'thread_id': 'thread_2', 'checkpoint_ns': '', 'checkpoint_id': '1f06b834-8374-697c-8002-7968d96cfb40'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_state(\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"thread_2\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6f0bf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the temperature in chennai now?', additional_kwargs={}, response_metadata={}, id='bf134153-8ff1-44eb-b83b-3c8005751959'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'dfdw5kk4m', 'function': {'arguments': '{\"query\":\"current temperature in chennai now\",\"topic\":\"news\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2351, 'total_tokens': 2378, 'completion_time': 0.069227334, 'prompt_time': 0.242482064, 'queue_time': 0.05240523600000002, 'total_time': 0.311709398}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--31acbf1e-b461-486d-8a1c-c6bc947f6cc2-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current temperature in chennai now', 'topic': 'news'}, 'id': 'dfdw5kk4m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2351, 'output_tokens': 27, 'total_tokens': 2378}),\n",
       "  ToolMessage(content='{\"query\": \"current temperature in chennai now\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.accuweather.com/en/in/chennai/206671/current-weather/206671\", \"title\": \"Chennai, Tamil Nadu, India Current Weather - AccuWeather\", \"content\": \"Chennai, Tamil Nadu · Current Weather. 6:35 AM. 83°F. Cloudy. RealFeel® 88°. Very Warm. RealFeel Guide. Very Warm. 82° to 89°. Older\", \"score\": 0.8396422, \"raw_content\": null}, {\"url\": \"https://weather.com/en-IN/weather/today/l/Chennai+Tamil+Nadu?canonicalCityId=e8f12d57533a42fc2b834f36c6195432\", \"title\": \"Weather forecast and conditions for Chennai, Tamil Nadu\", \"content\": \"Weather Today in Chennai, Tamil Nadu. Feels Like32°. 05:52. 18:37. High/Low. 35°/28°. Wind. 18 km/h. Humidity. 68%. Dew Point. 22°. Pressure. 1004.4 mb.\", \"score\": 0.72207695, \"raw_content\": null}], \"response_time\": 1.2}', name='tavily_search', id='1b0c2bb7-7a79-47ba-a363-fd09f0f728ba', tool_call_id='dfdw5kk4m'),\n",
       "  AIMessage(content='The current temperature in Chennai is around 83°F (28°C) according to AccuWeather.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 2683, 'total_tokens': 2704, 'completion_time': 0.048071421, 'prompt_time': 0.211209251, 'queue_time': 0.05282709800000002, 'total_time': 0.259280672}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0564c4c5-fdc1-41bf-a213-17043f4e29ad-0', usage_metadata={'input_tokens': 2683, 'output_tokens': 21, 'total_tokens': 2704}),\n",
       "  HumanMessage(content='What is my last question?', additional_kwargs={}, response_metadata={}, id='8a08f485-18a3-4592-b7ba-b3928297e2aa'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'abp2t3fvh', 'function': {'arguments': '{}', 'name': 'date_time'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 2719, 'total_tokens': 2728, 'completion_time': 0.025335316, 'prompt_time': 0.507659999, 'queue_time': 0.05574564999999998, 'total_time': 0.532995315}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9fe4e1be-92ed-4b9e-8acd-825054c3efc4-0', tool_calls=[{'name': 'date_time', 'args': {}, 'id': 'abp2t3fvh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2719, 'output_tokens': 9, 'total_tokens': 2728}),\n",
       "  ToolMessage(content='2025-07-28 12:51:44, PM', name='date_time', id='0fbee984-e59f-422a-80e1-463524266090', tool_call_id='abp2t3fvh'),\n",
       "  AIMessage(content='Your last question was \"What is my last question?\" which was asked at 2025-07-28 12:51:44 PM.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 2752, 'total_tokens': 2783, 'completion_time': 0.128490918, 'prompt_time': 0.016080801, 'queue_time': 0.05632941, 'total_time': 0.144571719}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e6400d02-8012-4efd-a369-bf9af062d2db-0', usage_metadata={'input_tokens': 2752, 'output_tokens': 31, 'total_tokens': 2783})]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"What is my last question?\")\n",
    "        ],\n",
    "    ),\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"thread_2\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcfd6a",
   "metadata": {},
   "source": [
    "### Since InMemorySaver lost all the memory, We SQLiteSaver to make it persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a065f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000234B7BF6D50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph, add_messages\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# LLM initialization.\n",
    "llm = ChatGroq(\n",
    "    groq_api_key = settings.groq_api_key,\n",
    "    temperature = 0,\n",
    "    max_tokens = 3000,\n",
    "    model = \"deepseek-r1-distill-llama-70b\" \n",
    ")\n",
    "\n",
    "# Bind all the custom tools to the LLM.\n",
    "llm_with_tools = llm.bind_tools(tools = available_tools)\n",
    "# Create a ToolNode to execute appropriate tools.\n",
    "tool_node: ToolNode = ToolNode(\n",
    "    tools = available_tools,\n",
    "    messages_key = \"messages\"\n",
    ")\n",
    "\n",
    "CHATBOT = \"chatbot\"\n",
    "TOOLS = \"tools\"\n",
    "\n",
    "\n",
    "class BasicChatBotState(TypedDict):\n",
    "    \"Represents the state of basic ChatBot\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def chatbot_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Execute tools and return the results.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke(state[\"messages\"])]\n",
    "    }\n",
    "\n",
    "def tool_executor_node(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    tool_response: List[ToolMessage] = tool_node.invoke(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": tool_response\n",
    "    }    \n",
    "\n",
    "def tool_call_required(state: BasicChatBotState) -> BasicChatBotState:\n",
    "    \"\"\"Check if the last message requires tool calls.\"\"\"\n",
    "    last_ai_message: AIMessage = state[\"messages\"][-1]\n",
    "    if last_ai_message.tool_calls:\n",
    "        return TOOLS\n",
    "    return END \n",
    "\n",
    "\n",
    "# Build the Graph Workflow.\n",
    "graph_builder = StateGraph(BasicChatBotState)\n",
    "graph_builder.add_node(CHATBOT, chatbot_node)\n",
    "graph_builder.add_node(TOOLS, tool_executor_node)\n",
    "graph_builder.set_entry_point(CHATBOT)\n",
    "graph_builder.add_edge(TOOLS, CHATBOT)\n",
    "graph_builder.add_conditional_edges(\n",
    "    CHATBOT,\n",
    "    tool_call_required,\n",
    "    path_map = {\n",
    "        TOOLS: TOOLS,\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(CHATBOT, END)\n",
    "\n",
    "# Memory\n",
    "conn: sqlite3.Connection = sqlite3.connect(\n",
    "    \"checkpoints.sqlite\", \n",
    "    check_same_thread = False\n",
    ")\n",
    "\n",
    "memory = SqliteSaver(conn)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer = memory)\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "45a7bf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, this is Nagarjun. You?\n",
      "\n",
      "AI: Hello, Nagarjun! How can I assist you today?\n",
      "\n",
      "Human: what is the result of 89 * 1.5\n",
      "\n",
      "AI: The result of 89 multiplied by 1.5 is 133.5.\n",
      "\n",
      "Human: what is my last question?\n",
      "\n",
      "AI: Your last question was about calculating the result of 89 multiplied by 1.5.\n",
      "\n",
      "Human: exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input: str = input(\"Human: \")\n",
    "    print(f\"Human: {user_input}\", end = \"\\n\\n\")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"end\"]:\n",
    "        break\n",
    "    response: BasicChatBotState = graph.invoke(\n",
    "        BasicChatBotState(\n",
    "            messages = [\n",
    "                HumanMessage(content = user_input)\n",
    "            ],\n",
    "        ), \n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"first_thread\"\n",
    "            }\n",
    "        } \n",
    "    ) \n",
    "    answer: str = response[\"messages\"][-1].content\n",
    "    print(f\"AI: {answer}\", end = \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbbff35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, this is Nagarjun. You?', additional_kwargs={}, response_metadata={}, id='65a4c432-2851-40c1-8fc4-b24819fe9813'),\n",
       "  AIMessage(content='Hello, Nagarjun! How can I assist you today?', additional_kwargs={'reasoning_content': 'Alright, the user greeted me with \"Hi, this is Nagarjun. You?\" which seems like a friendly hello. Since I\\'m an AI, I should respond in a welcoming manner. I want to make sure they feel comfortable asking for help. I\\'ll keep it simple and open-ended to encourage them to share what they need assistance with.\\n'}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 2107, 'total_tokens': 2193, 'completion_time': 0.440426181, 'prompt_time': 0.159865975, 'queue_time': 0.05606192800000001, 'total_time': 0.600292156}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ee2a873c-af0b-4bc8-b67e-29c7e5d02817-0', usage_metadata={'input_tokens': 2107, 'output_tokens': 86, 'total_tokens': 2193}),\n",
       "  HumanMessage(content='what is the result of 89 * 1.5', additional_kwargs={}, response_metadata={}, id='c2c42b67-7183-4938-9747-97c51f6bb4c1'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, so the user asked, \"what is the result of 89 * 1.5\". I need to figure out how to respond. First, I should recognize that this is a multiplication problem. Since I have a tool called \"multiplication\" available, that seems like the right one to use.\\n\\nLooking at the function parameters, it requires two numbers, num_1 and num_2. The user provided 89 and 1.5, so I\\'ll plug those into the arguments. I should make sure to format the JSON correctly, with both numbers as floats to handle the decimal.\\n\\nI don\\'t think any other tools are needed here since it\\'s a straightforward calculation. So, I\\'ll structure the tool call with the multiplication function and the given numbers. That should give the user the correct result.\\n', 'tool_calls': [{'id': 'nxqex1cc6', 'function': {'arguments': '{\"num_1\":89,\"num_2\":1.5}', 'name': 'multiplication'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 204, 'prompt_tokens': 2134, 'total_tokens': 2338, 'completion_time': 1.035742669, 'prompt_time': 0.155070203, 'queue_time': 0.06391532700000002, 'total_time': 1.190812872}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--81f47878-9506-42c8-9ebf-f3153659e35b-0', tool_calls=[{'name': 'multiplication', 'args': {'num_1': 89, 'num_2': 1.5}, 'id': 'nxqex1cc6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2134, 'output_tokens': 204, 'total_tokens': 2338}),\n",
       "  ToolMessage(content='133.5', name='multiplication', id='5666db1c-69b8-43d0-a308-1c3b19db64fd', tool_call_id='nxqex1cc6'),\n",
       "  AIMessage(content='The result of 89 multiplied by 1.5 is 133.5.', additional_kwargs={'reasoning_content': \"Alright, so I need to figure out the result of multiplying 89 by 1.5. Hmm, okay, let's break this down step by step. First, I know that multiplying by 1.5 is the same as multiplying by 1 and then adding half of the original number. So, if I take 89 and multiply it by 1, I just get 89. Then, I need to find half of 89. Half of 80 is 40, and half of 9 is 4.5, so adding those together gives me 44.5. Now, if I add that to the original 89, I get 89 + 44.5, which equals 133.5. So, the result of 89 multiplied by 1.5 is 133.5.\\n\\nAlternatively, I could think of 1.5 as 3/2. So, multiplying 89 by 3 gives me 267, and then dividing that by 2 results in 133.5. Both methods lead me to the same answer, which is reassuring.\\n\\nI could also use the standard multiplication method. Multiplying 89 by 1.5 involves multiplying 89 by 1 and 89 by 0.5 separately and then adding the two results. 89 times 1 is 89, and 89 times 0.5 is 44.5. Adding them together gives me 133.5. This consistent result across different methods makes me confident that 133.5 is the correct answer.\\n\\nTo double-check, I might use a calculator to verify the multiplication. Typing in 89 multiplied by 1.5 should give me 133.5, which matches what I calculated manually. This confirmation helps ensure that there were no arithmetic errors in my calculations.\\n\\nIn summary, whether I break down the multiplication into simpler parts, use fractions, or apply the standard multiplication algorithm, I consistently arrive at the same result: 133.5. This thorough approach helps me be certain that the answer is accurate.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 444, 'prompt_tokens': 2196, 'total_tokens': 2640, 'completion_time': 1.6831061360000001, 'prompt_time': 0.160243699, 'queue_time': 0.05840994100000002, 'total_time': 1.8433498350000002}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2e50db3a-1e7b-4849-8ea7-1a82f76ed257-0', usage_metadata={'input_tokens': 2196, 'output_tokens': 444, 'total_tokens': 2640}),\n",
       "  HumanMessage(content='what is my last question?', additional_kwargs={}, response_metadata={}, id='68098631-8b80-4674-8802-18ba2a8b1b5c'),\n",
       "  AIMessage(content='Your last question was about calculating the result of 89 multiplied by 1.5.', additional_kwargs={'reasoning_content': 'Alright, the user is asking, \"what is my last question?\" I need to recall the conversation history to determine what their last query was. Looking back, the user previously asked about multiplying 89 by 1.5. So, their last question was about that calculation. I should provide a clear and concise answer indicating that their last question was regarding the multiplication of those two numbers.\\n'}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 2222, 'total_tokens': 2323, 'completion_time': 0.477508334, 'prompt_time': 0.167519953, 'queue_time': 0.058326427000000014, 'total_time': 0.645028287}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d8a40c6e-6541-4b67-a398-ead40d1b2315-0', usage_metadata={'input_tokens': 2222, 'output_tokens': 101, 'total_tokens': 2323}),\n",
       "  HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}, id='fab1b855-b1b0-446b-907d-22d7be09a251'),\n",
       "  AIMessage(content='Yes, I remember your name is Nagarjun! How can I assist you today?', additional_kwargs={'reasoning_content': \"Alright, the user is asking if I remember their name. Looking back at the conversation history, the user introduced themselves as Nagarjun at the very beginning. So, I should confirm that I remember their name and address them by it to make the interaction more personal and friendly. I'll respond positively and use their name to show that I'm paying attention and value their interaction.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 2249, 'total_tokens': 2346, 'completion_time': 0.470695658, 'prompt_time': 0.170281093, 'queue_time': 0.05447449700000001, 'total_time': 0.640976751}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9e82cbf7-195e-4a21-a6f1-783e696598fe-0', usage_metadata={'input_tokens': 2249, 'output_tokens': 97, 'total_tokens': 2346}),\n",
       "  HumanMessage(content='When coolie movie will releases?', additional_kwargs={}, response_metadata={}, id='f4989eca-fdcc-4cea-b647-6a5a5deac5b3'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Alright, the user is asking about the release date of the \"Coolie\" movie. I need to find this information for them. Since I don\\'t have real-time data, I\\'ll use the Tavily search tool to look it up.\\n\\nFirst, I\\'ll set up the search query with \"Coolie movie release date\" to get the most relevant results. I\\'ll keep the search depth as \"advanced\" to ensure comprehensive results. I\\'ll also set the topic to \"news\" since movie releases are typically covered in entertainment news. Additionally, I\\'ll include the favicon to get any relevant icons, but I won\\'t include images since they aren\\'t necessary for this query.\\n\\nI\\'ll leave the start and end dates as null to get the most up-to-date information without restricting the timeframe. No specific domains are needed here, so I\\'ll leave those fields empty as well. The time range is set to null because the release date could be recent or upcoming, and I want the search to cover all possibilities.\\n\\nBy using these parameters, I can provide the user with the latest and most accurate information about the \"Coolie\" movie release date.\\n', 'tool_calls': [{'id': '97yehxnxh', 'function': {'arguments': '{\"include_favicon\":true,\"query\":\"Coolie movie release date\",\"search_depth\":\"advanced\",\"topic\":\"news\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 2276, 'total_tokens': 2551, 'completion_time': 1.381485764, 'prompt_time': 0.178126157, 'queue_time': 0.071651153, 'total_time': 1.5596119210000001}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1fb7bad2-8fd2-41b0-8c64-e5abc2f9ead2-0', tool_calls=[{'name': 'tavily_search', 'args': {'include_favicon': True, 'query': 'Coolie movie release date', 'search_depth': 'advanced', 'topic': 'news'}, 'id': '97yehxnxh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2276, 'output_tokens': 275, 'total_tokens': 2551}),\n",
       "  ToolMessage(content='{\"query\": \"Coolie movie release date\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://en.wikipedia.org/wiki/Coolie_(2025_film)\", \"title\": \"Coolie (2025 film) - Wikipedia\", \"content\": \"_Coolie_ is scheduled to release worldwide on 14 August 2025 in standard and IMAX formats.\\\\n\\\\nCast\\\\n----\\\\n\\\\n[edit&action=edit&section=1 \\\\\"Edit section: Cast\\\\\")]\\\\n\\\\n   Rajinikanth as Deva\\\\n   Nagarjuna \\\\\"Nagarjuna (actor)\\\\\") as Simon\\\\n   Upendra \\\\\"Upendra (actor)\\\\\") as Kaleesha\\\\n   Soubin Shahir as Dayal\\\\n   Sathyaraj as Rajasekar\\\\n   Shruti Haasan as Preethi\\\\n   Reba Monica John\\\\n   Junior MGR\\\\n   Monisha Blessy\\\\n   Pooja Hegde (special appearance in a song)\\\\n   Aamir Khan (cameo appearance)\\\\n\\\\nProduction\\\\n---------- [...] Release\\\\n-------\\\\n\\\\n[edit&action=edit&section=9 \\\\\"Edit section: Release\\\\\")]\\\\n\\\\n### Theatrical\\\\n\\\\n[edit&action=edit&section=10 \\\\\"Edit section: Theatrical\\\\\")]\\\\n\\\\n_Coolie_ is scheduled to release theatrically on 14 August 2025, a day before Independence Day \\\\\"Independence Day (India)\\\\\"),( in standard and IMAX formats.( Apart from the original Tamil language, the film is also scheduled to release with dubbed versions in the Telugu, Hindi and Kannada languages.(\\\\n\\\\n### Home media [...] | Release date |  14 August 2025(2025-08-14) |\\\\n| Country | India |\\\\n| Language | Tamil |\\\\n| Budget | est.₹350 crore( |\", \"score\": 0.9035075, \"raw_content\": null, \"favicon\": \"https://en.wikipedia.org/static/favicon/wikipedia.ico\"}, {\"url\": \"https://www.imdb.com/title/tt27441481/\", \"title\": \"Coolie (2025) - IMDb\", \"content\": \"Among the Indian movies with planned releases for the rest of the year, these titles were consistently the most popular with IMDb customers between January 1, 2025 and July 1, 2025, as determined by the actual page views of the more than 250 million monthly visitors to IMDb worldwide. [...] # Coolie\\\\n\\\\nCoolie (2025)\\\\nGet ready for the most anticipated Indian movies of the second half of 2025! Major films are set to release, including Aankhon Ki Gustaakhiyan, War 2, Coolie, Son of Sardaar 2, and more. Here\\'s a list of the top picks that everyone is eagerly awaiting! [...] ### Videos1\\\\n\\\\nMost Anticipated Indian Movies: July-December 2025\\\\n\\\\n### Photos74\\\\n\\\\nView Poster\\\\nView Poster\\\\nView Poster\\\\nView Poster\\\\nView Poster\\\\nView Poster\\\\n\\\\n### Top cast18\\\\n\\\\nAamir Khan\\\\nRajinikanth\\\\nPooja Hegde\\\\nNagarjuna Akkineni\\\\nSoubin Shahir\\\\nShruti Haasan\\\\nUpendra\\\\nReba Monica John\\\\nKishore Kumar G.\\\\nSathyaraj\\\\nKanna Ravi\\\\nMonisha Blessy\\\\nKaali Venkat\\\\nAyyappa P. Sharma\\\\n\\\\n## Anmol Jamwal\\'s Most Anticipated Movies of 2025\\\\n\\\\n## Anmol Jamwal\\'s Most Anticipated Movies of 2025\\\\n\\\\nPoster\\\\n\\\\n### More like this\", \"score\": 0.7959917, \"raw_content\": null, \"favicon\": \"https://m.media-amazon.com/images/G/01/imdb/images-ANDW73HA/favicon_iPhone_retina_180x180._CB1582158069_UX152_.png\"}], \"response_time\": 7.87}', name='tavily_search', id='a8415cfa-a6a6-4291-927e-a76f51b3f7e1', tool_call_id='97yehxnxh'),\n",
       "  AIMessage(content='The movie *Coolie* is scheduled to release on **August 14, 2025**, in theaters. This information is confirmed by multiple sources, including Wikipedia and IMDb. The film will be released in Tamil, with dubbed versions in Telugu, Hindi, and Kannada.', additional_kwargs={'reasoning_content': 'Alright, the user is asking about the release date of the \"Coolie\" movie. I remember they mentioned their name is Nagarjun, so I should address them by name to make it personal.\\n\\nI need to use the Tavily search tool to find the latest information. The query should be straightforward: \"Coolie movie release date\". Since this is a specific query about an upcoming movie, setting the search depth to \"advanced\" makes sense to get more detailed results. Also, categorizing the topic as \"news\" will help fetch relevant and timely information.\\n\\nLooking at the tool\\'s response, the first result from Wikipedia clearly states the release date as August 14, 2025. IMDb also confirms this date, which adds credibility. Both sources mention the film\\'s cast and languages it will be released in, which might be additional useful information for the user.\\n\\nI should present the release date prominently and mention the sources to assure the user of the information\\'s reliability. Keeping the response friendly and clear will make it helpful for Nagarjun.\\n'}, response_metadata={'token_usage': {'completion_tokens': 272, 'prompt_tokens': 3194, 'total_tokens': 3466, 'completion_time': 1.355337263, 'prompt_time': 0.272467187, 'queue_time': 0.140230225, 'total_time': 1.62780445}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--615253cd-86f7-4b68-b9f9-d9d620469249-0', usage_metadata={'input_tokens': 3194, 'output_tokens': 272, 'total_tokens': 3466})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask for the name, and test whether it remembers.\n",
    "\n",
    "response: BasicChatBotState = graph.invoke(\n",
    "    BasicChatBotState(\n",
    "        messages = [\n",
    "            HumanMessage(\"When coolie movie will releases?\")\n",
    "        ]\n",
    "    ),\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"first_thread\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd6b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie *Coolie* is scheduled to release on **August 14, 2025**, in theaters. This information is confirmed by multiple sources, including Wikipedia and IMDb. The film will be released in Tamil, with dubbed versions in Telugu, Hindi, and Kannada.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759d802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
